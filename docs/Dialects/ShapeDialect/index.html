<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'shape' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.64.1"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/master/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/master/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li></ul></nav></div><div class=content-container><main><h1>'shape' Dialect</h1><p>Types and operations for shape dialect</p><p>This dialect contains operations for shape inference.</p><p>Note: Unless explicitly stated, all functions that return a shape and take
shapes as input, return the invalid shape if one of its operands is an
invalid shape. This avoids flagging multiple errors for one verification
failure. The dialect itself does not specify how errors should be combined
(there are multiple different options, from always choosing first operand,
concatting etc. on how to combine them).</p><p><nav id=TableOfContents><ul><li><a href=#type-definition>Type definition</a><ul><li><a href=#component-type>component type</a></li><li><a href=#element-type>element type</a></li><li><a href=#shape>shape</a></li><li><a href=#size>size</a></li><li><a href=#value-shape>value shape</a></li></ul></li><li><a href=#operation-definition>Operation definition</a><ul><li><a href=#shapeadd-shapeaddop>shape.add (shape::AddOp)</a></li><li><a href=#shapebroadcast-shapebroadcastop>shape.broadcast (shape::BroadcastOp)</a></li><li><a href=#shapeconcat-shapeconcatop>shape.concat (shape::ConcatOp)</a></li><li><a href=#shapeconstant-shapeconstantop>shape.constant (shape::ConstantOp)</a></li><li><a href=#shapedebug_print-shapedebugprintop>shape.debug_print (shape::DebugPrintOp)</a></li><li><a href=#shapefrom_extent_tensor-shapefromextenttensorop>shape.from_extent_tensor (shape::FromExtentTensorOp)</a></li><li><a href=#shapejoin-shapejoinop>shape.join (shape::JoinOp)</a></li><li><a href=#shapemul-shapemulop>shape.mul (shape::MulOp)</a></li><li><a href=#shapereduce-shapereduceop>shape.reduce (shape::ReduceOp)</a></li><li><a href=#shapeshape_of-shapeshapeofop>shape.shape_of (shape::ShapeOfOp)</a></li><li><a href=#shapesplit_at-shapesplitatop>shape.split_at (shape::SplitAtOp)</a></li><li><a href=#shapeto_tensor-shapetoextenttensorop>shape.to_tensor (shape::ToExtentTensorOp)</a></li><li><a href=#shapeyield-shapeyieldop>shape.yield (shape::YieldOp)</a></li></ul></li></ul></nav><h2 id=type-definition>Type definition</h2><h3 id=component-type>component type</h3><p><code>shape.element_type</code> represents the element type of the ShapedType. It may
be unknown, error or regular element type supported by ShapedType.</p><h3 id=element-type>element type</h3><p><code>shape.element_type</code> represents the element type of the ShapedType. It may
be unknown, error or regular element type supported by ShapedType.</p><h3 id=shape>shape</h3><p><code>shape.type</code> represents either an unranked shape, a ranked shape with
possibly unknown dimensions or an invalid shape. The rank is of type
<code>shape.size</code> and, if rank is known, the extent is a 1D tensor of type
<code>shape.size</code>.</p><p>Shape is printed:</p><ul><li><code>[*]</code> if it is an unranked shape</li><li><code>[?, 2]</code> if a rank 2 tensor with one unknown dimension</li><li><code>[3, 4]</code> is a rank 2 static tensor</li><li><code>[]</code> is a scalar</li><li><code>[1]</code> is a rank 1 tensor with 1 element</li><li><code>[invalid]</code> for an invalid shape</li></ul><h3 id=size>size</h3><p><code>shape.size</code> represents a non-negative integer with support for being
unknown and invalid.</p><p>Operations on <code>shape.size</code> types are specialized to handle unknown/dynamic
value. So, for example, <code>&lt;unknown> + x == &lt;unknown></code> for all non-error <code>x : !shape.size</code> (e.g., an unknown value does not become known due to addition).</p><h3 id=value-shape>value shape</h3><p><code>shape.value_shape</code> represents the value produced by an operation (this
corresponds to <code>Value</code> in the compiler) and a shape. Conceptually this is a
tuple of a value (potentially unknown) and <code>shape.type</code>. The value and shape
can either or both be unknown. If both the <code>value</code> and <code>shape</code> are known,
then the shape of <code>value</code> is conformant with <code>shape</code>.</p><h2 id=operation-definition>Operation definition</h2><h3 id=shapeadd-shapeaddop><code>shape.add</code> (shape::AddOp)</h3><p>Addition of sizes</p><p>Adds two valid sizes as follows:</p><ul><li>lhs + rhs = unknown if either lhs or rhs unknown;</li><li>lhs + rhs = (int)lhs + (int)rhs if known;</li></ul><h4 id=operands>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>size</td></tr><tr><td align=center><code>rhs</code></td><td>size</td></tr></tbody></table><h4 id=results>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>size</td></tr></tbody></table><h3 id=shapebroadcast-shapebroadcastop><code>shape.broadcast</code> (shape::BroadcastOp)</h3><p>Returns the broadcasted output shape of two inputs</p><p>Computes the broadcasted output shape following:</p><ol><li><p>If any inputs are unranked, output is unranked;</p></li><li><p>Else the input array with number of dimensions smaller than the max
input dimension, has 1â€™s prepended to its shapes and the output shape is
calculated as follows:</p><pre><code>output[i] = lhs[i] if lhs[i] == rhs[i] or rhs[i] is unknown/undefined
          = rhs[i] if lhs[i] is unknown/undefined
          = lhs[i] if rhs[i] == 1
          = rhs[i] if lhs[i] == 1
          = error  if lhs[i] != rhs[i]
</code></pre></li></ol><p>Op has an optional string attribute for the error case where there is no
broadcastable output shape possible for the given inputs.</p><h4 id=attributes>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>error</code></td><td align=center>StringAttr</td><td>string attribute</td></tr></tbody></table><h4 id=operands-1>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>shape</td></tr><tr><td align=center><code>rhs</code></td><td>shape</td></tr></tbody></table><h4 id=results-1>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape</td></tr></tbody></table><h3 id=shapeconcat-shapeconcatop><code>shape.concat</code> (shape::ConcatOp)</h3><p>Concatenates two shapes.</p><p>Creates a shape whose dimensions consist of first the dimensions from <code>lhs</code>
followed by the dimensions of <code>rhs</code>.</p><p>Example:
concat([2,3], [4,5]) -> [2,3,4,5]
concat([], []) -> []
concat([], [4,5,6]) -> [4,5,6]</p><h4 id=operands-2>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>shape</td></tr><tr><td align=center><code>rhs</code></td><td>shape</td></tr></tbody></table><h4 id=results-2>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape</td></tr></tbody></table><h3 id=shapeconstant-shapeconstantop><code>shape.constant</code> (shape::ConstantOp)</h3><p>Creates a shape constant</p><p>An operation that builds a size or shape from integer or array attribute.
It allows for creating dynamically valued shapes by using <code>?</code> for unknown
values. A constant shape specified with <code>*</code> will return an unranked shape.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%x</span> <span class=p>=</span> shape<span class=p>.</span><span class=kt>constant</span> <span class=m>10</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size
</code></pre></div><h4 id=attributes-1>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>value</code></td><td align=center>Attribute</td><td>any attribute</td></tr></tbody></table><h4 id=results-3>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapedebug_print-shapedebugprintop><code>shape.debug_print</code> (shape::DebugPrintOp)</h3><p>Prints the input shape or size</p><p>Prints the input dim or shape and passes through input.</p><p>Note: This is intended for testing and debugging only.</p><h4 id=operands-3>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>input</code></td><td>shape or size</td></tr></tbody></table><h4 id=results-4>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>output</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapefrom_extent_tensor-shapefromextenttensorop><code>shape.from_extent_tensor</code> (shape::FromExtentTensorOp)</h3><p>Creates a shape from a tensor of extents</p><p>Creates a shape from a 1D integral tensor of extents. The rank of the
resulting shape equals the number of elements in the tensor, and the
extents match the values of the elements.</p><h4 id=operands-4>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>input</code></td><td>tensor of 32-bit signless integer values</td></tr></tbody></table><h4 id=results-5>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape</td></tr></tbody></table><h3 id=shapejoin-shapejoinop><code>shape.join</code> (shape::JoinOp)</h3><p>Returns the least general shape.size of its operands</p><p>An operation that computes the least general shape of input operands. This
effectively asserts that corresponding static dimensions are equal. The
behavior is to match each element of the <code>shape.type</code> and propagate the most
restrictive information, returning an invalid shape if there are
contradictory requirements. E.g., using pseudo code</p><pre><code>shape.join([*], [*]) -&gt; [*]
shape.join([*], [1, ?]) -&gt; [1, ?]
shape.join([1, 2], [1, ?]) -&gt; [1, 2]
shape.join([*], [1, 2]) -&gt; [1, 2]
shape.join([], []) -&gt; []
shape.join([], [*]) -&gt; []
shape.join([], [?, ?]) -&gt; [invalid]
shape.join([1, ?], [2, ?, ?]) -&gt; [invalid]
</code></pre><p><code>shape.join</code> also allows specifying an optional error string, that may be
used to return an error to the user upon mismatch of dimensions.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c</span> <span class=p>=</span> shape<span class=p>.</span>join <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;&lt;reason&gt;&#34;</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>type
</code></pre></div><h4 id=attributes-2>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>error</code></td><td align=center>StringAttr</td><td>string attribute</td></tr></tbody></table><h4 id=operands-5>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>arg0</code></td><td>shape or size</td></tr><tr><td align=center><code>arg1</code></td><td>shape or size</td></tr></tbody></table><h4 id=results-6>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapemul-shapemulop><code>shape.mul</code> (shape::MulOp)</h3><p>Multiplication of sizes</p><p>Multiplies two valid sizes as follows:</p><ul><li>lhs * rhs = unknown if either lhs or rhs unknown;</li><li>lhs * rhs = (int)lhs * (int)rhs if both known;</li></ul><h4 id=operands-6>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>size</td></tr><tr><td align=center><code>rhs</code></td><td>size</td></tr></tbody></table><h4 id=results-7>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>size</td></tr></tbody></table><h3 id=shapereduce-shapereduceop><code>shape.reduce</code> (shape::ReduceOp)</h3><p>Returns an expression reduced over a shape</p><p>An operation that takes as input a shape, number of initial values and has a
region/function that is applied repeatedly for every dimension of the shape.</p><p>Conceptually this op performs the following reduction:</p><pre><code>res[] = init;
for (int i = 0, e = shape.rank(); i != e; ++i) {
  res = fn(i, shape[i], res[0], ..., res[n]);
}
</code></pre><p>Where fn is provided by the user and the result of the reduce op is the
last computed output of the reduce function. As an example, computing the
number of elements</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=kt>func</span> <span class=nf>@shape_num_elements</span><span class=p>(</span><span class=nv>%shape</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>type<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size <span class=p>{</span>
  <span class=nv>%0</span> <span class=p>=</span> <span class=s>&#34;shape.constant_dim&#34;</span><span class=p>(</span><span class=p>)</span> <span class=p>{</span><span class=nl>value =</span> <span class=m>1</span> <span class=p>:</span> <span class=k>i32</span><span class=p>}</span> <span class=p>:</span> <span class=p>(</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
  <span class=nv>%1</span> <span class=p>=</span> <span class=s>&#34;shape.reduce&#34;</span><span class=p>(</span><span class=nv>%shape</span><span class=p>,</span> <span class=nv>%0</span><span class=p>)</span> <span class=p>(</span> <span class=p>{</span>
    <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%index</span><span class=p>:</span> <span class=k>i32</span><span class=p>,</span> <span class=nv>%dim</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=nv>%lci</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span><span class=p>:</span>
      <span class=nv>%acc</span> <span class=p>=</span> <span class=s>&#34;shape.mul&#34;</span><span class=p>(</span><span class=nv>%lci</span><span class=p>,</span> <span class=nv>%dim</span><span class=p>)</span> <span class=p>:</span>
        <span class=p>(</span><span class=p>!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
      shape<span class=p>.</span>yield <span class=nv>%acc</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size
    <span class=p>}</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=p>!</span>shape<span class=p>.</span>type<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span><span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span>
  <span class=kt>return</span> <span class=nv>%1</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size
<span class=p>}</span>
</code></pre></div><p>If the shape is unranked, then the results of the op is also unranked.</p><h4 id=operands-7>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>shape</code></td><td>shape</td></tr><tr><td align=center><code>args</code></td><td>any type</td></tr></tbody></table><h4 id=results-8>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>any type</td></tr></tbody></table><h3 id=shapeshape_of-shapeshapeofop><code>shape.shape_of</code> (shape::ShapeOfOp)</h3><p>Returns shape of a value or shaped type operand</p><h4 id=operands-8>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>arg</code></td><td>shaped of any type values or value shape</td></tr></tbody></table><h4 id=results-9>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>shape</td></tr></tbody></table><h3 id=shapesplit_at-shapesplitatop><code>shape.split_at</code> (shape::SplitAtOp)</h3><p>Splits a shape at a given index.</p><p>Splits a shape at a given dimension <code>index</code>, returning two shapes.
If <code>index</code> is negative, it is treated as indexing from the back of the
shape. This negative-handling behavior is important when handling unranked
shapes, where the positive index is not necessarily knowable due to a
dynamic number of leading dimensions.</p><p>Examples:</p><ul><li>split_at([4,5,6], index=0) -> [], [4,5,6]</li><li>split_at([4,5,6], index=1) -> [4], [5,6]</li><li>split_at([4,5,6], index=2) -> [4,5], [6]</li><li>split_at([4,5,6], index=3) -> [4,5,6], []</li><li>split_at([4,5,6], index=4) -> error</li><li>split_at([4,5,6], index=-1) -> [4,5], [6]</li><li>split_at([4,5,6], index=-2) -> [4], [5,6]</li><li>split_at([4,5,6], index=-3) -> [], [4,5,6]</li><li>split_at([4,5,6], index=-4) -> error</li></ul><p>Requires:</p><ul><li><code>index</code> is in the range [-rank(operand),rank(operand)]</li></ul><h4 id=operands-9>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>operand</code></td><td>shape</td></tr><tr><td align=center><code>index</code></td><td>32-bit signless integer</td></tr></tbody></table><h4 id=results-10>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>head</code></td><td>shape</td></tr><tr><td align=center><code>tail</code></td><td>shape</td></tr></tbody></table><h3 id=shapeto_tensor-shapetoextenttensorop><code>shape.to_tensor</code> (shape::ToExtentTensorOp)</h3><p>Creates a dimension tensor from a shape</p><p>Converts a shape to a 1D integral tensor of extents. The number of elements
in the tensor equals the rank of the shape, and the elements equal the
extents of the shape.</p><p>If the shape represents an error, then this op currently aborts the program.</p><h4 id=operands-10>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>input</code></td><td>shape</td></tr></tbody></table><h4 id=results-11>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>tensor of 32-bit signless integer values</td></tr></tbody></table><h3 id=shapeyield-shapeyieldop><code>shape.yield</code> (shape::YieldOp)</h3><p>Returns the value to parent op</p><p>Syntax:</p><pre><code>operation ::= `shape.yield` attr-dict ($operands^ `:` type($operands))?
</code></pre><h4 id=operands-11>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>operands</code></td><td>any type</td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Dialects/ROCDLDialect/ title="'rocdl' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - 'rocdl' Dialect</a>
<a class="nav nav-next" href=/docs/Dialects/SPIR-V/ title="'spv' Dialect">Next - 'spv' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/talks/>Talks and Related Publications</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class="parent has-sub-menu"><a href=/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=/docs/Dialects/Linalg/>'linalg' Dialect</a></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/LoopDialect/>'loop' Dialect</a></li><li><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li class=active><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>'spv' Dialect</a></li><li><a href=/docs/Dialects/Standard/>'std' Dialect</a></li><li><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li></ul></li><li class=has-sub-menu><a href=/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Rationale/UsageOfConst/>Usage of 'Const' in MLIR, for core IR types</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Tutorial Introduction</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/Tutorials/DefiningAttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li></ul></li><li><a href=/docs/EDSC/>Background: declarative builders API</a></li><li><a href=/docs/ConversionToLLVMDialect/>Conversion to the LLVM Dialect</a></li><li><a href=/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li><a href=/docs/GenericDAGRewriter/>MLIR Generic DAG Rewriter Infrastructure</a></li><li><a href=/docs/Interfaces/>MLIR Interfaces</a></li><li><a href=/docs/LangRef/>MLIR Language Reference</a></li><li><a href=/docs/Traits/>MLIR Operation Traits</a></li><li><a href=/docs/Passes/>MLIR Passes</a></li><li><a href=/docs/Quantization/>MLIR Quantization</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=/docs/ShapeInference/>Shape Inference</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/OpDefinitions/>Table-driven Operation Definition Specification (ODS)</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>
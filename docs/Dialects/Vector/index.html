<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'vector' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.64.1"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/Vector/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/master/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/master/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li></ul></nav></div><div class=content-container><main><h1>'vector' Dialect</h1><p><nav id=TableOfContents><ul><li><a href=#positioning-in-the-codegen-infrastructure>Positioning in the Codegen Infrastructure</a></li><li><a href=#components-of-a-generic-retargetable-vector-level-dialect>Components of a Generic Retargetable Vector-Level Dialect</a></li><li><a href=#short-description-of-the-existing-infrastructure>Short Description of the Existing Infrastructure</a><ul><li><a href=#llvm-level>LLVM level</a></li><li><a href=#hardware-vector-ops>Hardware Vector Ops</a></li><li><a href=#virtual-vector-ops>Virtual Vector Ops</a></li><li><a href=#virtual-vector-rewrite-patterns>Virtual Vector Rewrite Patterns</a></li><li><a href=#virtual-vector-to-hardware-vector-lowering>Virtual Vector to Hardware Vector Lowering</a></li></ul></li><li><a href=#rationale>Rationale</a><ul><li><a href=#hardware-as-vector-machines-of-minimum-granularity>Hardware as vector Machines of Minimum Granularity</a></li><li><a href=#transformations-problems-avoided>Transformations Problems Avoided</a></li><li><a href=#the-big-out-of-scope-piece-automatic-vectorization>The Big Out-Of-Scope Piece: Automatic Vectorization</a></li></ul></li><li><a href=#bikeshed-naming-discussion>Bikeshed Naming Discussion</a></li><li><a href=#deeperdive>DeeperDive</a><ul><li><a href=#alternatives-for-lowering-an-n-d-vector-type-to-llvm>Alternatives For Lowering an n-D Vector Type to LLVM</a></li><li><a href=#constraints-inherited-from-llvm-see-langref>Constraints Inherited from LLVM (see LangRef)</a></li><li><a href=#nested-aggregate>Nested Aggregate</a></li><li><a href=#flattened-1-d-vector-type>Flattened 1-D Vector Type</a></li><li><a href=#discussion>Discussion</a></li><li><a href=#relationship-to-llvm-matrix-type-proposal>Relationship to LLVM matrix type proposal.</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li><li><a href=#operations>Operations</a><ul><li><a href=#vectorbroadcast-vectorbroadcastop>vector.broadcast (vector::BroadcastOp)</a></li><li><a href=#vectorconstant_mask-vectorconstantmaskop>vector.constant_mask (vector::ConstantMaskOp)</a></li><li><a href=#vectorcontract-vectorcontractionop>vector.contract (vector::ContractionOp)</a></li><li><a href=#vectorcreate_mask-vectorcreatemaskop>vector.create_mask (vector::CreateMaskOp)</a></li><li><a href=#vectorextractelement-vectorextractelementop>vector.extractelement (vector::ExtractElementOp)</a></li><li><a href=#vectorextract-vectorextractop>vector.extract (vector::ExtractOp)</a></li><li><a href=#vectorextract_slices-vectorextractslicesop>vector.extract_slices (vector::ExtractSlicesOp)</a></li><li><a href=#vectorfma-vectorfmaop>vector.fma (vector::FMAOp)</a></li><li><a href=#vectorinsertelement-vectorinsertelementop>vector.insertelement (vector::InsertElementOp)</a></li><li><a href=#vectorinsert-vectorinsertop>vector.insert (vector::InsertOp)</a></li><li><a href=#vectorinsert_slices-vectorinsertslicesop>vector.insert_slices (vector::InsertSlicesOp)</a></li><li><a href=#vectorinsert_strided_slice-vectorinsertstridedsliceop>vector.insert_strided_slice (vector::InsertStridedSliceOp)</a></li><li><a href=#vectormatrix_multiply-vectormatmulop>vector.matrix_multiply (vector::MatmulOp)</a></li><li><a href=#vectorouterproduct-vectorouterproductop>vector.outerproduct (vector::OuterProductOp)</a></li><li><a href=#vectorprint-vectorprintop>vector.print (vector::PrintOp)</a></li><li><a href=#vectorreduction-vectorreductionop>vector.reduction (vector::ReductionOp)</a></li><li><a href=#vectorreshape-vectorreshapeop>vector.reshape (vector::ReshapeOp)</a></li><li><a href=#vectorshape_cast-vectorshapecastop>vector.shape_cast (vector::ShapeCastOp)</a></li><li><a href=#vectorshuffle-vectorshuffleop>vector.shuffle (vector::ShuffleOp)</a></li><li><a href=#vectorstrided_slice-vectorstridedsliceop>vector.strided_slice (vector::StridedSliceOp)</a></li><li><a href=#vectortransfer_read-vectortransferreadop>vector.transfer_read (vector::TransferReadOp)</a></li><li><a href=#vectortransfer_write-vectortransferwriteop>vector.transfer_write (vector::TransferWriteOp)</a></li><li><a href=#vectortranspose-vectortransposeop>vector.transpose (vector::TransposeOp)</a></li><li><a href=#vectortuple_get-vectortuplegetop>vector.tuple_get (vector::TupleGetOp)</a></li><li><a href=#vectortuple-vectortupleop>vector.tuple (vector::TupleOp)</a></li><li><a href=#vectortype_cast-vectortypecastop>vector.type_cast (vector::TypeCastOp)</a></li></ul></li></ul></nav><p>MLIR supports multi-dimensional <code>vector</code> types and custom operations on those
types. A generic, retargetable, higher-order <code>vector</code> type (<code>n-D</code> with <code>n > 1</code>) is a structured type, that carries semantic information useful for
transformations. This document discusses retargetable abstractions that exist
in MLIR today and operate on ssa-values of type <code>vector</code> along with pattern
rewrites and lowerings that enable targeting specific instructions on concrete
targets. These abstractions serve to separate concerns between operations on
<code>memref</code> (a.k.a buffers) and operations on <code>vector</code> values. This is not a
new proposal but rather a textual documentation of existing MLIR components
along with a rationale.</p><h2 id=positioning-in-the-codegen-infrastructure>Positioning in the Codegen Infrastructure</h2><p>The following diagram, recently presented with the
<a href=https://drive.google.com/corp/drive/u/0/folders/1sRAsgsd8Bvpm_IxREmZf2agsGU2KvrK->StructuredOps
abstractions</a>
,
captures the current codegen paths implemented in MLIR in the various existing
lowering paths.
<img src=https://user-images.githubusercontent.com/10148468/71177417-f78e4d80-2239-11ea-92ef-700f42ea503f.png alt></p><p>The following diagram seeks to isolate <code>vector</code> dialects from the complexity
of the codegen paths and focus on the payload-carrying ops that operate on std
and <code>vector</code> types. This diagram is not to be taken as set in stone and
representative of what exists today but rather illustrates the layering of
abstractions in MLIR.</p><p><img src=https://user-images.githubusercontent.com/10148468/71176949-e85ad000-2238-11ea-9806-200843bc4943.png alt="vector Abstractions in MLIR"></p><p>This  separates concerns related to (a) defining efficient operations on
<code>vector</code> types from (b) program analyses + transformations on <code>memref</code>, loops
and other types of structured ops (be they <code>HLO</code>, <code>LHLO</code>, <code>Linalg</code> or other ).
Looking a bit forward in time, we can put a stake in the ground and venture
that the higher level of <code>vector</code>-level primitives we build and target from
codegen (or some user/language level), the simpler our task will be, the more
complex patterns can be expressed and the better performance will be.</p><h2 id=components-of-a-generic-retargetable-vector-level-dialect>Components of a Generic Retargetable Vector-Level Dialect</h2><p>The existing MLIR <code>vector</code>-level dialects are related to the following
bottom-up abstractions:</p><ol><li>Representation in <code>LLVMIR</code> via data structures, instructions and
intrinsics. This is referred to as the <code>LLVM</code> level.</li><li>Set of machine-specific operations and types that are built to translate
almost 1-1 with the HW ISA. This is referred to as the Hardware Vector level;
a.k.a <code>HWV</code>. For instance, we have (a) a <code>NVVM</code> dialect (for <code>CUDA</code>) with
tensor core ops, (b) accelerator-specific dialects (internal), a potential
(future) <code>CPU</code> dialect to capture <code>LLVM</code> intrinsics more closely and other
dialects for specific hardware. Ideally this should be auto-generated as much
as possible from the <code>LLVM</code> level.</li><li>Set of virtual, machine-agnostic, operations that are informed by costs at
the <code>HWV</code>-level. This is referred to as the Virtual Vector level; a.k.a
<code>VV</code>. This is the level that higher-level abstractions (codegen, automatic
vectorization, potential vector language, &mldr;) targets.</li></ol><p>The existing generic, retargetable, <code>vector</code>-level dialect is related to the
following top-down rewrites and conversions:</p><ol><li>MLIR Rewrite Patterns applied by the MLIR <code>PatternRewrite</code> infrastructure
to progressively lower to implementations that match closer and closer to the
<code>HWV</code>. Some patterns are &ldquo;in-dialect&rdquo; <code>VV -> VV</code> and some are conversions <code>VV -> HWV</code>.</li><li><code>Virtual Vector -> Hardware Vector</code> lowering is specified as a set of MLIR
lowering patterns that are specified manually for now.</li><li><code>Hardware Vector -> LLVM</code> lowering is a mechanical process that is written
manually at the moment and that should be automated, following the <code>LLVM -> Hardware Vector</code> ops generation as closely as possible.</li></ol><h2 id=short-description-of-the-existing-infrastructure>Short Description of the Existing Infrastructure</h2><h3 id=llvm-level>LLVM level</h3><p>On CPU, the <code>n-D</code> <code>vector</code> type currently lowers to
<code>!llvm&lt;array&lt;vector>></code>. More concretely, <code>vector&lt;4x8x128xf32></code> lowers to
<code>!llvm&lt;[4 x [ 8 x [ 128 x float ]]]></code>.
There are tradeoffs involved related to how one can access subvectors and how
one uses <code>llvm.extractelement</code>, <code>llvm.insertelement</code> and
<code>llvm.shufflevector</code>. A
<a href=#DeeperDive>deeper dive section</a>
discusses the
current lowering choices and tradeoffs.</p><h3 id=hardware-vector-ops>Hardware Vector Ops</h3><p>Hardware Vector Ops are implemented as one dialect per target.
For internal hardware, we are auto-generating the specific HW dialects.
For <code>GPU</code>, the <code>NVVM</code> dialect adds operations such as <code>mma.sync</code>, <code>shfl</code> and
tests.
For <code>CPU</code> things are somewhat in-flight because the abstraction is close to
<code>LLVMIR</code>. The jury is still out on  whether a generic <code>CPU</code> dialect is
concretely needed, but it seems reasonable to have the same levels of
abstraction for all targets and perform cost-based lowering decisions in MLIR
even for <code>LLVM</code>.
Specialized <code>CPU</code> dialects that would capture specific features not well
captured by LLVM peephole optimizations of on different types that core MLIR
supports (e.g. Scalable Vectors) are welcome future extensions.</p><h3 id=virtual-vector-ops>Virtual Vector Ops</h3><p>Some existing Standard and Vector Dialect on <code>n-D</code> <code>vector</code> types comprise:</p><pre><code>%2 = std.addf %0, %1 : vector&lt;3x7x8xf32&gt;  // -&gt; vector&lt;3x7x8xf32&gt;
%2 = std.mulf %0, %1 : vector&lt;3x7x8xf32&gt;  // -&gt; vector&lt;3x7x8xf32&gt;
%2 = std.splat %1    : vector&lt;3x7x8xf32&gt;  // -&gt; vector&lt;3x7x8xf32&gt;

%1 = vector.extract %0[1]: vector&lt;3x7x8xf32&gt;                 // -&gt; vector&lt;7x8xf32&gt;
%1 = vector.extract %0[1, 5]: vector&lt;3x7x8xf32&gt;            // -&gt; vector&lt;8xf32&gt;
%2 = vector.outerproduct %0, %1: vector&lt;4xf32&gt;, vector&lt;8xf32&gt;     // -&gt; vector&lt;4x8xf32&gt;
%3 = vector.outerproduct %0, %1, %2: vector&lt;4xf32&gt;, vector&lt;8xf32&gt; // fma when adding %2
%3 = vector.strided_slice %0 {offsets = [2, 2], sizes = [2, 2], strides = [1, 1]}:
   vector&lt;4x8x16xf32&gt; // Returns a slice of type vector&lt;2x2x16xf32&gt;

%2 = vector.transfer_read %A[%0, %1]
  {permutation_map = (d0, d1) -&gt; (d0)}: memref&lt;7x?xf32&gt;, vector&lt;4xf32&gt;

vector.transfer_write %f1, %A[%i0, %i1, %i2, %i3]
  {permutation_map = (d0, d1, d2, d3) -&gt; (d3, d1, d0)} :
    vector&lt;5x4x3xf32&gt;, memref&lt;?x?x?x?xf32&gt;
</code></pre><p>The list of Vector is currently undergoing evolutions and is best kept
track of by following the evolution of the
<a href=https://github.com/llvm/llvm-project/blob/master/mlir/include/mlir/Dialect/Vector/VectorOps.td>VectorOps.td</a>
ODS file (markdown documentation is automatically generated locally when
building and populates the
<a href=https://github.com/llvm/llvm-project/blob/master/mlir/docs/Dialects/Vector.md>Vector
doc</a>
). Recent
extensions are driven by concrete use cases of interest. A notable such use
case is the <code>vector.contract</code> op which applies principles of the StructuredOps
abstraction to <code>vector</code> types.</p><h3 id=virtual-vector-rewrite-patterns>Virtual Vector Rewrite Patterns</h3><p>The following rewrite patterns exist at the <code>VV->VV</code> level:</p><ol><li>The now retired <code>MaterializeVector</code> pass used to legalize ops on a
coarse-grained virtual <code>vector</code> to a finer-grained virtual <code>vector</code> by
unrolling. This has been rewritten as a retargetable unroll-and-jam pattern on
<code>vector</code> ops and <code>vector</code> types.</li><li>The lowering of <code>vector_transfer</code> ops legalizes <code>vector</code> load/store ops to
permuted loops over scalar load/stores. This should evolve to loops over
<code>vector</code> load/stores + <code>mask</code> operations as they become available <code>vector</code> ops
at the <code>VV</code> level.</li></ol><p>The general direction is to add more Virtual Vector level ops and implement
more useful <code>VV -> VV</code> rewrites as composable patterns that the PatternRewrite
infrastructure can apply iteratively.</p><h3 id=virtual-vector-to-hardware-vector-lowering>Virtual Vector to Hardware Vector Lowering</h3><p>For now, <code>VV -> HWV</code> are specified in C++ (see for instance the
<a href=https://github.com/tensorflow/mlir/commit/0a0c4867c6a6fcb0a2f17ef26a791c1d551fe33d>SplatOpLowering for n-D
vectors</a>
or the
<a href=https://github.com/tensorflow/mlir/commit/957b1ca9680b4aacabb3a480fbc4ebd2506334b8>VectorOuterProductOp
lowering</a>
).</p><p>Simple
<a href=https://github.com/llvm/llvm-project/blob/master/mlir/test/Conversion/VectorToLLVM/vector-to-llvm.mlir>conversion
tests</a>
are available for the <code>LLVM</code> target starting from the Virtual Vector Level.</p><h2 id=rationale>Rationale</h2><h3 id=hardware-as-vector-machines-of-minimum-granularity>Hardware as <code>vector</code> Machines of Minimum Granularity</h3><p>Higher-dimensional <code>vector</code>s are ubiquitous in modern HPC hardware. One way to
think about Generic Retargetable <code>vector</code>-Level Dialect is that it operates on
<code>vector</code> types that are a multiples of a &ldquo;good&rdquo; <code>vector</code> size so the HW can
efficiently implement a set of high-level primitives
(e.g. <code>vector&lt;8x8x8x16xf32></code> when HW <code>vector</code> size is say <code>vector&lt;4x8xf32></code>).</p><p>Some notable <code>vector</code> sizes of interest include:</p><ol><li>CPU: <code>vector&lt;HW_vector_size * k></code>, <code>vector&lt;core_count * k’ x HW_vector_size * k></code> and <code>vector&lt;socket_count x core_count * k’ x HW_vector_size * k></code></li><li>GPU: <code>vector&lt;warp_size * k></code>, <code>vector&lt;warp_size * k x float4></code> and
<code>vector&lt;warp_size * k x 4 x 4 x 4></code> for tensor_core sizes,</li><li>Other accelerators: n-D <code>vector</code> as first-class citizens in the HW.</li></ol><p>Depending on the target, ops on sizes that are not multiples of the HW
<code>vector</code> size may either produce slow code (e.g. by going through <code>LLVM</code>
legalization) or may not legalize at all (e.g. some unsupported accelerator X
combination of ops and types).</p><h3 id=transformations-problems-avoided>Transformations Problems Avoided</h3><p>A <code>vector&lt;16x32x64xf32></code> virtual <code>vector</code> is a coarse-grained type that can be
“unrolled” to HW-specific sizes. The multi-dimensional unrolling factors are
carried in the IR by the <code>vector</code> type. After unrolling, traditional
instruction-level scheduling can be run.</p><p>The following key transformations (along with the supporting analyses and
structural constraints) are completely avoided by operating on a <code>vector</code>
<code>ssa-value</code> abstraction:</p><ol><li>Loop unroll and unroll-and-jam.</li><li>Loop and load-store restructuring for register reuse.</li><li>Load to store forwarding and Mem2reg.</li><li>Coarsening (raising) from finer-grained <code>vector</code> form.</li></ol><p>Note that “unrolling” in the context of <code>vector</code>s corresponds to partial loop
unroll-and-jam and not full unrolling. As a consequence this is expected to
compose with SW pipelining where applicable and does not result in ICache blow
up.</p><h3 id=the-big-out-of-scope-piece-automatic-vectorization>The Big Out-Of-Scope Piece: Automatic Vectorization</h3><p>One important piece not discussed here is automatic vectorization
(automatically raising from scalar to n-D <code>vector</code> ops and types). The TL;DR
is that when the first &ldquo;super-vectorization&rdquo; prototype was implemented, MLIR
was nowhere near as mature as it is today. As we continue building more
abstractions in <code>VV -> HWV</code>, there is an opportunity to revisit vectorization
in MLIR.</p><p>Since this topic touches on codegen abstractions, it is technically out of the
scope of this survey document but there is a lot to discuss in light of
structured op type representations and how a vectorization transformation can
be reused across dialects. In particular, MLIR allows the definition of
dialects at arbitrary levels of granularity and lends itself favorably to
progressive lowering. The argument can be made that automatic vectorization on
a loops + ops abstraction is akin to raising structural information that has
been lost. Instead, it is possible to revisit vectorization as simple pattern
rewrites, provided the IR is in a suitable form. For instance, vectorizing a
<code>linalg.generic</code> op whose semantics match a <code>matmul</code> can be done
<a href=https://github.com/tensorflow/mlir/commit/bff722d6b59ab99b998f0c2b9fccd0267d9f93b5>quite easily
with a
pattern</a>
. In
fact this pattern is trivial to generalize to any type of contraction when
targeting the <code>vector.contract</code> op, as well as to any field (<code>+/*</code>, <code>min/+</code>,
<code>max/+</code>, <code>or/and</code>, <code>logsumexp/+</code> &mldr;) . In other words, by operating on a
higher level of generic abstractions than affine loops, non-trivial
transformations become significantly simpler and composable at a finer
granularity.</p><p>Irrespective of the existence of an auto-vectorizer, one can build a notional
vector language based on the VectorOps dialect and build end-to-end models
with expressing <code>vector</code>s in the IR directly and simple
pattern-rewrites.
<a href=https://github.com/llvm/llvm-project/blob/master/mlir/docs/EDSC.md>EDSC</a>
s
provide a simple way of driving such a notional language directly in C++.</p><h2 id=bikeshed-naming-discussion>Bikeshed Naming Discussion</h2><p>There are arguments against naming an n-D level of abstraction <code>vector</code>
because most people associate it with 1-D <code>vector</code>s. On the other hand,
<code>vector</code>s are first-class n-D values in MLIR.
The alternative name Tile has been proposed, which conveys higher-D
meaning. But it also is one of the most overloaded terms in compilers and
hardware.
For now, we generally use the <code>n-D</code> <code>vector</code> name and are open to better
suggestions.</p><h2 id=deeperdive>DeeperDive</h2><p>This section describes the tradeoffs involved in lowering the MLIR n-D vector
type and operations on it to LLVM-IR. Putting aside the
<a href=http://lists.llvm.org/pipermail/llvm-dev/2018-October/126871.html>LLVM
Matrix</a>
proposal for now, this assumes LLVM only has built-in support for 1-D
vector. The relationship with the LLVM Matrix proposal is discussed at the end
of this document.</p><p>MLIR does not currently support dynamic vector sizes (i.e. SVE style) so the
discussion is limited to static rank and static vector sizes
(e.g. <code>vector&lt;4x8x16x32xf32></code>). This section discusses operations on vectors
in LLVM and MLIR.</p><p>LLVM instructions are prefixed by the <code>llvm.</code> dialect prefix
(e.g. <code>llvm.insertvalue</code>). Such ops operate exclusively on 1-D vectors and
aggregates following the
<a href=https://llvm.org/docs/LangRef.html>LLVM LangRef</a>
.
MLIR operations are prefixed by the <code>vector.</code> dialect prefix
(e.g. <code>vector.insertelement</code>). Such ops operate exclusively on MLIR <code>n-D</code>
<code>vector</code> types.</p><h3 id=alternatives-for-lowering-an-n-d-vector-type-to-llvm>Alternatives For Lowering an n-D Vector Type to LLVM</h3><p>Consider a vector of rank n with static sizes <code>{s_0, ... s_{n-1}}</code> (i.e. an
MLIR <code>vector&lt;s_0x...s_{n-1}xf32></code>). Lowering such an <code>n-D</code> MLIR vector type to
an LLVM descriptor can be done by either:</p><ol><li>Flattening to a <code>1-D</code> vector: <code>!llvm&lt;"(s_0*...*s_{n-1})xfloat"></code> in the
MLIR LLVM dialect.</li><li>Nested aggregate type of <code>1-D</code> vector:
<code>!llvm&lt;"[s_0x[s_1x[...&lt;s_{n-1}xfloat>]]]"></code> in the MLIR LLVM dialect.</li><li>A mix of both.</li></ol><p>There are multiple tradeoffs involved in choosing one or the other that we
discuss. It is important to note that “a mix of both” immediately reduces to
“nested aggregate type of 1-D vector” with a <code>vector.cast %0: vector&lt;4x8x16x32xf32> to vector&lt;4x4096xf32></code> operation, that flattens the most
&ldquo;k&rdquo; minor dimensions.</p><h3 id=constraints-inherited-from-llvm-see-langref>Constraints Inherited from LLVM (see LangRef)</h3><p>The first constraint was already mentioned: LLVM only supports <code>1-D</code> <code>vector</code>
types natively.
Additional constraints are related to the difference in LLVM between vector
and aggregate types:</p><pre><code> “Aggregate Types are a subset of derived types that can contain multiple
 member types. Arrays and structs are aggregate types. Vectors are not
 considered to be aggregate types.”.
</code></pre><p>This distinction is also reflected in some of the operations. For <code>1-D</code>
vectors, the operations <code>llvm.extractelement</code>, <code>llvm.insertelement</code>, and
<code>llvm.shufflevector</code> apply, with direct support for dynamic indices. For <code>n-D</code>
vectors with <code>n>1</code>, and thus aggregate types at LLVM level, the more
restrictive operations <code>llvm.extractvalue</code> and <code>llvm.insertvalue</code> apply, which
only accept static indices. There is no direct shuffling support for aggregate
types.</p><p>The next sentence illustrates a recurrent tradeoff, also found in MLIR,
between “value types” (subject to SSA use-def chains) and “memory types”
(subject to aliasing and side-effects):</p><pre><code>“Structures in memory are accessed using ‘load’ and ‘store’ by getting a
pointer to a field with the llvm.getelementptr instruction. Structures in
registers are accessed using the llvm.extractvalue and llvm.insertvalue
instructions.”
</code></pre><p>When transposing this to MLIR, <code>llvm.getelementptr</code> works on pointers to <code>n-D</code>
vectors in memory. For <code>n-D</code>, vectors values that live in registers we can use
<code>vector.extract</code> and <code>vector.insert</code> which do not accept dynamic indices. Note
that this is consistent with hardware considerations as discussed below.</p><p>An alternative is to use an LLVM <code>1-D</code> <code>vector</code> type for which one can use
<code>llvm.extractelement</code>, <code>llvm.insertelement</code> and <code>llvm.shufflevector</code>. These
operations accept dynamic indices. The implication is that one has to use a
flattened lowering of an MLIR n-D vector to an LLVM 1-D vector.</p><p>There are multiple tradeoffs involved that mix implications on the programming
model, execution on actual HW and what is visible or hidden from codegen. They
are discussed in the following sections.</p><h3 id=nested-aggregate>Nested Aggregate</h3><p>Pros:</p><ol><li>Natural encoding n-D vector -> (n-1)-D aggregate over 1-D vector.</li><li>No need for linearization / delinearization logic inserted everywhere.</li><li><code>llvm.insertvalue</code>, <code>llvm.extractvalue</code> of <code>(n-k)-D</code> aggregate is natural.</li><li><code>llvm.insertelement</code>, <code>llvm.extractelement</code>, <code>llvm.shufflevector</code> over
<code>1-D</code> vector type is natural.</li></ol><p>Cons:</p><ol><li><code>llvm.insertvalue</code> / <code>llvm.extractvalue</code> does not accept dynamic indices
but only static ones.</li><li>Dynamic indexing on the non-most-minor dimension requires roundtrips to
memory.</li><li>Special intrinsics and native instructions in LLVM operate on <code>1-D</code>
vectors. This is not expected to be a practical limitation thanks to a
<code>vector.cast %0: vector&lt;4x8x16x32xf32> to vector&lt;4x4096xf32></code> operation, that
flattens the most minor dimensions (see the bigger picture in implications on
codegen).</li></ol><h3 id=flattened-1-d-vector-type>Flattened 1-D Vector Type</h3><p>Pros:</p><ol><li><code>insertelement</code> / <code>extractelement</code> / <code>shufflevector</code> with dynamic indexing
is possible over the whole lowered <code>n-D</code> vector type.</li><li>Supports special intrinsics and native operations.</li></ol><p>Cons:</p><ol><li>Requires linearization/delinearization logic everywhere, translations are
complex.</li><li>Hides away the real HW structure behind dynamic indexing: at the end of the
day, HW vector sizes are generally fixed and multiple vectors will be needed
to hold a vector that is larger than the HW.</li><li>Unlikely peephole optimizations will result in good code: arbitrary dynamic
accesses, especially at HW vector boundaries unlikely to result in regular
patterns.</li></ol><h3 id=discussion>Discussion</h3><h4 id=hw-vectors-and-implications-on-the-sw-and-the-programming-model>HW Vectors and Implications on the SW and the Programming Model</h4><p>As of today, the LLVM model only support <code>1-D</code> vector types. This is
unsurprising because historically, the vast majority of HW only supports <code>1-D</code>
vector registers. We note that multiple HW vendors are in the process of
evolving to higher-dimensional physical vectors.</p><p>In the following discussion, let&rsquo;s assume the HW vector size is <code>1-D and the SW vector size is </code>n-D<code>, with </code>n >= 1<code>. The same discussion would apply with</code>2-D<code>HW</code>vector<code>size and</code>n >= 2<code>. In this context, most HW exhibit a vector register file. The number of such vectors is fixed. Depending on the rank and sizes of the SW vector abstraction and the HW vector sizes and number of registers, an </code>n-D<code>SW vector type may be materialized by a mix of multiple</code>1-D` HW vector registers + memory locations at a given
point in time.</p><p>The implication of the physical HW constraints on the programming model are
that one cannot index dynamically across hardware registers: a register file
can generally not be indexed dynamically. This is because the register number
is fixed and one either needs to unroll explicitly to obtain fixed register
numbers or go through memory. This is a constraint familiar to CUDA
programmers: when declaring a <code>private float a[4]</code>; and subsequently indexing
with a <em>dynamic</em> value results in so-called <strong>local memory</strong> usage
(i.e. roundtripping to memory).</p><h4 id=implication-on-codegen>Implication on codegen</h4><p>MLIR <code>n-D</code> vector types are currently represented as <code>(n-1)-D</code> arrays of <code>1-D</code>
vectors when lowered to LLVM.
This introduces the consequences on static vs dynamic indexing discussed
previously: <code>extractelement</code>, <code>insertelement</code> and <code>shufflevector</code> on <code>n-D</code>
vectors in MLIR only support static indices. Dynamic indices are only
supported on the most minor <code>1-D</code> vector but not the outer <code>(n-1)-D</code>.
For other cases, explicit load / stores are required.</p><p>The implications on codegen are as follows:</p><ol><li>Loops around <code>vector</code> values are indirect addressing of vector values, they
must operate on explicit load / store operations over <code>n-D</code> vector types.</li><li>Once an <code>n-D</code> <code>vector</code> type is loaded into an SSA value (that may or may
not live in <code>n</code> registers, with or without spilling, when eventually lowered),
it may be unrolled to smaller <code>k-D</code> <code>vector</code> types and operations that
correspond to the HW. This level of MLIR codegen is related to register
allocation and spilling that occur much later in the LLVM pipeline.</li><li>HW may support >1-D vectors with intrinsics for indirect addressing within
these vectors. These can be targeted thanks to explicit <code>vector_cast</code>
operations from MLIR <code>k-D</code> vector types and operations to LLVM <code>1-D</code> vectors +
intrinsics.</li></ol><p>Alternatively, we argue that directly lowering to a linearized abstraction
hides away the codegen complexities related to memory accesses by giving a
false impression of magical dynamic indexing across registers. Instead we
prefer to make those very explicit in MLIR and allow codegen to explore
tradeoffs.
Different HW will require different tradeoffs in the sizes involved in steps
1., 2. and 3.</p><p>Decisions made at the MLIR level will have implications at a much later stage
in LLVM (after register allocation). We do not envision to expose concerns
related to modeling of register allocation and spilling to MLIR
explicitly. Instead, each target will expose a set of &ldquo;good&rdquo; target operations
and <code>n-D</code> vector types, associated with costs that <code>PatterRewriters</code> at the
MLIR level will be able to target. Such costs at the MLIR level will be
abstract and used for ranking, not for accurate performance modeling. In the
future such costs will be learned.</p><h4 id=implication-on-lowering-to-accelerators>Implication on Lowering to Accelerators</h4><p>To target accelerators that support higher dimensional vectors natively, we
can start from either <code>1-D</code> or <code>n-D</code> vectors in MLIR and use <code>vector.cast</code> to
flatten the most minor dimensions to <code>1-D</code> <code>vector&lt;Kxf32></code> where <code>K</code> is an
appropriate constant. Then, the existing lowering to LLVM-IR immediately
applies, with extensions for accelerator-specific intrinsics.</p><p>It is the role of an Accelerator-specific vector dialect (see codegen flow in
the figure above) to lower the <code>vector.cast</code>. Accelerator -> LLVM lowering
would then consist of a bunch of <code>Accelerator -> Accelerator</code> rewrites to
perform the casts composed with <code>Accelerator -> LLVM</code> conversions + intrinsics
that operate on <code>1-D</code> <code>vector&lt;Kxf32></code>.</p><p>Some of those rewrites may need extra handling, especially if a reduction is
involved. For example, <code>vector.cast %0: vector&lt;K1x...xKnxf32> to vector&lt;Kxf32></code> when <code>K != K1 * … * Kn</code> and some arbitrary irregular
<code>vector.cast %0: vector&lt;4x4x17xf32> to vector&lt;Kxf32></code> may introduce masking
and intra-vector shuffling that may not be worthwhile or even feasible,
i.e. infinite cost.</p><p>However <code>vector.cast %0: vector&lt;K1x...xKnxf32> to vector&lt;Kxf32></code> when <code>K = K1 * … * Kn</code> should be close to a noop.</p><p>As we start building accelerator-specific abstractions, we hope to achieve
retargetable codegen: the same infra is used for CPU, GPU and accelerators
with extra MLIR patterns and costs.</p><h4 id=implication-on-calling-external-functions-that-operate-on-vectors>Implication on calling external functions that operate on vectors</h4><p>It is possible (likely) that we additionally need to linearize when calling an
external function.</p><h3 id=relationship-to-llvm-matrix-type-proposal>Relationship to LLVM matrix type proposal.</h3><p>The LLVM matrix proposal was formulated 1 year ago but seemed to be somewhat
stalled until recently. In its current form, it is limited to 2-D matrix types
and operations are implemented with LLVM intrinsics.
In contrast, MLIR sits at a higher level of abstraction and allows the
lowering of generic operations on generic n-D vector types from MLIR to
aggregates of 1-D LLVM vectors.
In the future, it could make sense to lower to the LLVM matrix abstraction
also for CPU even though MLIR will continue needing higher level abstractions.</p><p>On the other hand, one should note that as MLIR is moving to LLVM, this
document could become the unifying abstraction that people should target for</p><blockquote><p>1-D vectors and the LLVM matrix proposal can be viewed as a subset of this
work.</p></blockquote><h3 id=conclusion>Conclusion</h3><p>The flattened 1-D vector design in the LLVM matrix proposal is good in a
HW-specific world with special intrinsics. This is a good abstraction for
register allocation, Instruction-Level-Parallelism and
SoftWare-Pipelining/Modulo Scheduling optimizations at the register level.
However MLIR codegen operates at a higher level of abstraction where we want
to target operations on coarser-grained vectors than the HW size and on which
unroll-and-jam is applied and patterns across multiple HW vectors can be
matched.</p><p>This makes “nested aggregate type of 1-D vector” an appealing abstraction for
lowering from MLIR because:</p><ol><li>it does not hide complexity related to the buffer vs value semantics and
the memory subsystem and</li><li>it does not rely on LLVM to magically make all the things work from a too
low-level abstraction.</li></ol><p>The use of special intrinsics in a <code>1-D</code> LLVM world is still available thanks
to an explicit <code>vector.cast</code> op.</p><h2 id=operations>Operations</h2><h3 id=vectorbroadcast-vectorbroadcastop><code>vector.broadcast</code> (vector::BroadcastOp)</h3><p>broadcast operation</p><p>Syntax:</p><pre><code>operation ::= `vector.broadcast` $source attr-dict `:` type($source) `to` type($vector)
</code></pre><p>Broadcasts the scalar or k-D vector value in the source operand
to a n-D result vector such that the broadcast makes sense, i.e.,
the source operand is duplicated to match the given rank and sizes
in the result vector. The legality rules are:</p><ul><li>the source operand must have the same element type as the result type</li><li>a k-D vector &lt;s_1 x .. x s_k x type> can be broadcast to
a n-D vector &lt;t_1 x .. x t_n x type> if<ul><li>k &lt;= n, and</li><li>the sizes in the trailing dimensions n-k &lt; i &lt;= n with j=i+k-n
match exactly as s_j = t_i or s_j = 1:</li></ul><pre><code>    t_1 x   ..  t_n-k x t_n-k+1 x .. x t_i x .. x t_n
                        s_1     x .. x s_j x .. x s_k
        &lt;duplication&gt;         &lt;potential stretch&gt;
</code></pre></li></ul><p>The source operand is duplicated over all the missing leading dimensions
and stretched over the trailing dimensions where the source has a non-equal
dimension of 1. These rules imply that any scalar broadcast (k=0) to any
shaped vector with the same element type is always legal.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>0.0</span> <span class=p>:</span> <span class=k>f32</span>
<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>broadcast <span class=nv>%0</span> <span class=p>:</span> <span class=k>f32</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%2</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>broadcast <span class=nv>%1</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>source</code></td><td>any type</td></tr></tbody></table><h4 id=results>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorconstant_mask-vectorconstantmaskop><code>vector.constant_mask</code> (vector::ConstantMaskOp)</h3><p>creates a constant vector mask</p><p>Syntax:</p><pre><code>operation ::= `vector.constant_mask` $mask_dim_sizes attr-dict `:` type(results)
</code></pre><p>Creates and returns a vector mask where elements of the result vector
are set to &lsquo;0&rsquo; or &lsquo;1&rsquo;, based on whether the element indices are contained
within a hyper-rectangular region specified by the &lsquo;mask_dim_sizes&rsquo;
array attribute argument. Each element of the &lsquo;mask_dim_sizes&rsquo; array,
specifies an exclusive upper bound [0, mask-dim-size-element-value)
for a unique dimension in the vector result. The conjunction of the ranges
define a hyper-rectangular region within which elements values are set to 1
(otherwise element values are set to 0).</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// create a constant vector mask of size 4x3xi1 with elements in range
</span><span class=c></span><span class=c>// 0 &lt;= row &lt;= 2 and 0 &lt;= col &lt;= 1 are set to 1 (others to 0).
</span><span class=c></span><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span><span class=kt>constant</span>_mask <span class=p>[</span><span class=m>3</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>i1</span><span class=p>&gt;</span>

print <span class=nv>%1</span>
              columns
            <span class=m>0</span>    <span class=m>1</span>    <span class=m>2</span>
          <span class=err>|</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span>
        <span class=m>0</span> <span class=err>|</span> <span class=m>1</span>    <span class=m>1</span>    <span class=m>0</span>
  rows  <span class=m>1</span> <span class=err>|</span> <span class=m>1</span>    <span class=m>1</span>    <span class=m>0</span>
        <span class=m>2</span> <span class=err>|</span> <span class=m>1</span>    <span class=m>1</span>    <span class=m>0</span>
        <span class=m>3</span> <span class=err>|</span> <span class=m>0</span>    <span class=m>0</span>    <span class=m>0</span>
</code></pre></div><h4 id=attributes>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>mask_dim_sizes</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=results-1>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>vector of 1-bit signless integer values</td></tr></tbody></table><h3 id=vectorcontract-vectorcontractionop><code>vector.contract</code> (vector::ContractionOp)</h3><p>vector contraction operation</p><p>Computes the sum of products of vector elements along contracting
dimension pairs from 2 vectors of rank M and N respectively, adds this
intermediate result to the accumulator argument of rank K, and returns a
vector result of rank K (where K = num_lhs_free_dims + num_rhs_free_dims +
num_batch_dims (see dimension type descriptions below)). For K = 0 (no
free or batch dimensions), the accumulator and output are a scalar.</p><p>Optional vector mask arguments (produced by CreateMaskOp or ConstantMaskOp)
specify the dynamic dimension sizes of valid data within the lhs/rhs vector
arguments.</p><p>An iterator type attribute list must be specified, where each element of
the list represents an iterator with one of the following types:</p><p>*) &ldquo;reduction&rdquo;: reduction dimensions are present in the lhs and rhs
arguments but not in the output (and accumulator
argument). These are the dimensions along which the vector
contraction op computes the sum of products, and
contracting dimension pair dimension sizes must match
between lhs/rhs.
*) &ldquo;parallel&rdquo;: Batch dimensions are iterator type &ldquo;parallel&rdquo;, and
are non-contracting dimensions present in the lhs, rhs and
output. The lhs/rhs co-iterate along the batch dimensions,
which should be expressed in their indexing maps.</p><pre><code>           Free dimensions are iterator type &quot;parallel&quot;, and are
           non-contraction, non-batch dimensions accessed by either the
           lhs or rhs (but not both). The lhs and rhs free dimensions
           are unrelated to each other and do not co-iterate, which
           should be expressed in their indexing maps.
</code></pre><p>An indexing map attribute list must be specified with an entry for lhs, rhs
and acc arguments. An indexing map attribute specifies a mapping from each
iterator in the iterator type list, to each dimension of an N-D vector.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// Simple dot product (K = 0).
</span><span class=c></span><span class=nv>#contraction_accesses</span> <span class=p>=</span> <span class=p>[</span>
 affine_map<span class=p>&lt;</span><span class=p>(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>)</span><span class=p>&gt;</span><span class=p>,</span>
 affine_map<span class=p>&lt;</span><span class=p>(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>)</span><span class=p>&gt;</span><span class=p>,</span>
 affine_map<span class=p>&lt;</span><span class=p>(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span><span class=p>)</span><span class=p>&gt;</span>
<span class=p>]</span>
<span class=nv>#contraction_trait</span> <span class=p>=</span> <span class=p>{</span>
  <span class=nl>indexing_maps =</span> <span class=nv>#contraction_accesses</span><span class=p>,</span>
  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;reduction&#34;</span><span class=p>]</span>
<span class=p>}</span>
<span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>contract <span class=nv>#contraction_trait</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>10x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>10x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=k>f32</span>

<span class=c>// 2D vector contraction with one contracting dimension (matmul, K = 2).
</span><span class=c></span><span class=nv>#contraction_accesses</span> <span class=p>=</span> <span class=p>[</span>
  affine_map<span class=p>&lt;</span><span class=p>(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> k<span class=p>)</span><span class=p>&gt;</span><span class=p>,</span>
  affine_map<span class=p>&lt;</span><span class=p>(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>k<span class=p>,</span> j<span class=p>)</span><span class=p>&gt;</span><span class=p>,</span>
  affine_map<span class=p>&lt;</span><span class=p>(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)</span><span class=p>&gt;</span>
<span class=p>]</span>
<span class=nv>#contraction_trait</span> <span class=p>=</span> <span class=p>{</span>
  <span class=nl>indexing_maps =</span> <span class=nv>#contraction_accesses</span><span class=p>,</span>
  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;reduction&#34;</span><span class=p>]</span>
<span class=p>}</span>

<span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>contract <span class=nv>#contraction_trait</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x7x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x7x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=c>// 4D to 3D vector contraction with two contracting dimensions and
</span><span class=c></span><span class=c>// one batch dimension (K = 3).
</span><span class=c></span><span class=nv>#contraction_accesses</span> <span class=p>=</span> <span class=p>[</span>
  affine_map<span class=p>&lt;</span><span class=p>(</span>b0<span class=p>,</span> f0<span class=p>,</span> f1<span class=p>,</span> c0<span class=p>,</span> c1<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>c0<span class=p>,</span> b0<span class=p>,</span> c1<span class=p>,</span> f0<span class=p>)</span><span class=p>&gt;</span><span class=p>,</span>
  affine_map<span class=p>&lt;</span><span class=p>(</span>b0<span class=p>,</span> f0<span class=p>,</span> f1<span class=p>,</span> c0<span class=p>,</span> c1<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>b0<span class=p>,</span> c1<span class=p>,</span> c0<span class=p>,</span> f1<span class=p>)</span><span class=p>&gt;</span><span class=p>,</span>
  affine_map<span class=p>&lt;</span><span class=p>(</span>b0<span class=p>,</span> f0<span class=p>,</span> f1<span class=p>,</span> c0<span class=p>,</span> c1<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>b0<span class=p>,</span> f0<span class=p>,</span> f1<span class=p>)</span><span class=p>&gt;</span>
<span class=p>]</span>
<span class=nv>#contraction_trait</span> <span class=p>=</span> <span class=p>{</span>
  <span class=nl>indexing_maps =</span> <span class=nv>#contraction_accesses</span><span class=p>,</span>
  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span>
                    <span class=s>&#34;reduction&#34;</span><span class=p>,</span> <span class=s>&#34;reduction&#34;</span><span class=p>]</span>
<span class=p>}</span>

<span class=nv>%4</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>contract <span class=nv>#contraction_trait</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span>
    <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>7x8x16x15x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x16x7x5x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x15x5x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=c>// 4D vector contraction with two contracting dimensions and optional
</span><span class=c></span><span class=c>// vector mask arguments.
</span><span class=c></span><span class=nv>%lhs_mask</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span><span class=kt>constant</span>_mask <span class=p>[</span><span class=m>7</span><span class=p>,</span> <span class=m>8</span><span class=p>,</span> <span class=m>16</span><span class=p>,</span> <span class=m>15</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>7x8x16x15x</span><span class=k>i1</span><span class=p>&gt;</span>
<span class=nv>%rhs_mask</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span><span class=kt>constant</span>_mask <span class=p>[</span><span class=m>8</span><span class=p>,</span> <span class=m>16</span><span class=p>,</span> <span class=m>7</span><span class=p>,</span> <span class=m>5</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x16x7x5x</span><span class=k>i1</span><span class=p>&gt;</span>

<span class=nv>%5</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>contract <span class=nv>#contraction_trait</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>,</span> <span class=nv>%lhs_mask</span><span class=p>,</span> <span class=nv>%rhs_mask</span>
   <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>7x8x16x15x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x16x7x5x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x15x8x5x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-1>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>indexing_maps</code></td><td align=center>ArrayAttr</td><td>AffineMap array attribute</td></tr><tr><td align=center><code>iterator_types</code></td><td align=center>ArrayAttr</td><td>array attribute</td></tr></tbody></table><h4 id=operands-1>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>vector of any type values</td></tr><tr><td align=center><code>rhs</code></td><td>vector of any type values</td></tr><tr><td align=center><code>acc</code></td><td>any type</td></tr><tr><td align=center><code>masks</code></td><td>vector of 1-bit signless integer values</td></tr></tbody></table><h4 id=results-2>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>any type</td></tr></tbody></table><h3 id=vectorcreate_mask-vectorcreatemaskop><code>vector.create_mask</code> (vector::CreateMaskOp)</h3><p>creates a vector mask</p><p>Syntax:</p><pre><code>operation ::= `vector.create_mask` $operands attr-dict `:` type(results)
</code></pre><p>Creates and returns a vector mask where elements of the result vector
are set to &lsquo;0&rsquo; or &lsquo;1&rsquo;, based on whether the element indices are contained
within a hyper-rectangular region specified by the operands. Specifically,
each operand specifies a range [0, operand-value) for a unique dimension in
the vector result. The conjunction of the operand ranges define a
hyper-rectangular region within which elements values are set to 1
(otherwise element values are set to 0).</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// create a vector mask of size 4x3xi1 where elements in range
</span><span class=c></span><span class=c>// 0 &lt;= row &lt;= 2 and 0 &lt;= col &lt;= 1 are set to 1 (others to 0).
</span><span class=c></span><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>create_mask <span class=nv>%c3</span><span class=p>,</span> <span class=nv>%c2</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>i1</span><span class=p>&gt;</span>

print <span class=nv>%1</span>
              columns
            <span class=m>0</span>    <span class=m>1</span>    <span class=m>2</span>
          <span class=err>|</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span><span class=err>-</span>
        <span class=m>0</span> <span class=err>|</span> <span class=m>1</span>    <span class=m>1</span>    <span class=m>0</span>
  rows  <span class=m>1</span> <span class=err>|</span> <span class=m>1</span>    <span class=m>1</span>    <span class=m>0</span>
        <span class=m>2</span> <span class=err>|</span> <span class=m>1</span>    <span class=m>1</span>    <span class=m>0</span>
        <span class=m>3</span> <span class=err>|</span> <span class=m>0</span>    <span class=m>0</span>    <span class=m>0</span>
</code></pre></div><h4 id=operands-2>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>operands</code></td><td>index</td></tr></tbody></table><h4 id=results-3>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>vector of 1-bit signless integer values</td></tr></tbody></table><h3 id=vectorextractelement-vectorextractelementop><code>vector.extractelement</code> (vector::ExtractElementOp)</h3><p>extractelement operation</p><p>Syntax:</p><pre><code>operation ::= `vector.extractelement` $vector `[` $position `:` type($position) `]` attr-dict `:` type($vector)
</code></pre><p>Takes an 1-D vector and a dynamic index position and extracts the
scalar at that position. Note that this instruction resembles
vector.extract, but is restricted to 1-D vectors and relaxed
to dynamic indices. It is meant to be closer to LLVM&rsquo;s version:
<a href=https://llvm.org/docs/LangRef.html#extractelement-instruction>https://llvm.org/docs/LangRef.html#extractelement-instruction</a></p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>15</span> <span class=p>:</span> <span class=k>i32</span>
<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extractelement <span class=nv>%0</span><span class=p>[</span><span class=nv>%c</span> <span class=p>:</span> <span class=k>i32</span><span class=p>]</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-3>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr><tr><td align=center><code>position</code></td><td>signless integer</td></tr></tbody></table><h4 id=results-4>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>any type</td></tr></tbody></table><h3 id=vectorextract-vectorextractop><code>vector.extract</code> (vector::ExtractOp)</h3><p>extract operation</p><p>Takes an n-D vector and a k-D position and extracts the (n-k)-D vector at
the proper position. Degenerates to an element type in the 0-D case.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extract <span class=nv>%0</span><span class=p>[</span><span class=m>3</span><span class=p>]</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%2</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extract <span class=nv>%0</span><span class=p>[</span><span class=m>3</span><span class=p>,</span> <span class=m>3</span><span class=p>,</span> <span class=m>3</span><span class=p>]</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-2>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>position</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-4>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-5>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>any type</td></tr></tbody></table><h3 id=vectorextract_slices-vectorextractslicesop><code>vector.extract_slices</code> (vector::ExtractSlicesOp)</h3><p>vector extract slices operation</p><p>Syntax:</p><pre><code>operation ::= `vector.extract_slices` $vector `,` $sizes `,` $strides attr-dict `:` type($vector) `into`
              type(results)
</code></pre><p>Takes an N-d vector and returns a tuple of vector slices of &lsquo;vector&rsquo;,
based on &lsquo;sizes&rsquo; and &lsquo;strides&rsquo; parameters.</p><p>The arguments &lsquo;sizes&rsquo; and &lsquo;strides&rsquo; represent a specification for
generating the unrolling of &lsquo;vector&rsquo; shape, which has all slices of shape
&lsquo;sizes&rsquo; except for slices at dimension boundaries when &lsquo;vector&rsquo; dimension
sizes are not a multiple of &lsquo;sizes&rsquo;.</p><p>Each slice is returned at the tuple element index corresponding to the
linear index of the slice w.r.t the unrolling scheme represented by &lsquo;sizes&rsquo;.
Currently, only unit strides are supported.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=p>.</span><span class=p>.</span><span class=p>.</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x2x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extract_slices <span class=nv>%0</span><span class=p>,</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x2x</span><span class=k>f32</span><span class=p>&gt;</span> into tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>

<span class=c>// Example with partial slices at dimension boundaries.
</span><span class=c></span><span class=nv>%2</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=p>.</span><span class=p>.</span><span class=p>.</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extract_slices <span class=nv>%2</span><span class=p>,</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>f32</span><span class=p>&gt;</span> into tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span>
                               <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-3>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>sizes</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr><tr><td align=center><code>strides</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-5>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-6>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>tuple with any combination of vector of any type values values</td></tr></tbody></table><h3 id=vectorfma-vectorfmaop><code>vector.fma</code> (vector::FMAOp)</h3><p>vector fused multiply-add</p><p>Syntax:</p><pre><code>operation ::= `vector.fma` $lhs `,` $rhs `,` $acc attr-dict `:` type($lhs)
</code></pre><p>Multiply-add expressions operate on n-D vectors and compute a fused
pointwise multiply-and-accumulate: <code>$result = </code>$lhs * $rhs + $acc<code>. All operands and result have the same vector type. The semantics of the operation correspond to those of the </code>llvm.fma<code>[intrinsic](https://llvm.org/docs/LangRef.html#int-fma). In the particular case of lowering to LLVM, this is guaranteed to lower to the </code>llvm.fma.*` intrinsic.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>fma <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-6>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>vector of any type values</td></tr><tr><td align=center><code>rhs</code></td><td>vector of any type values</td></tr><tr><td align=center><code>acc</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-7>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorinsertelement-vectorinsertelementop><code>vector.insertelement</code> (vector::InsertElementOp)</h3><p>insertelement operation</p><p>Syntax:</p><pre><code>operation ::= `vector.insertelement` $source `,` $dest `[` $position `:` type($position) `]` attr-dict `:`
              type($result)
</code></pre><p>Takes a scalar source, an 1-D destination vector and a dynamic index
position and inserts the source into the destination at the proper
position. Note that this instruction resembles vector.insert, but
is restricted to 1-D vectors and relaxed to dynamic indices. It is
meant to be closer to LLVM&rsquo;s version:
<a href=https://llvm.org/docs/LangRef.html#insertelement-instruction>https://llvm.org/docs/LangRef.html#insertelement-instruction</a></p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>15</span> <span class=p>:</span> <span class=k>i32</span>
<span class=nv>%f</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>0.0</span>f <span class=p>:</span> <span class=k>f32</span>
<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>insertelement <span class=nv>%f</span><span class=p>,</span> <span class=nv>%0</span><span class=p>[</span><span class=nv>%c</span> <span class=p>:</span> <span class=k>i32</span><span class=p>]</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-7>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>source</code></td><td>any type</td></tr><tr><td align=center><code>dest</code></td><td>vector of any type values</td></tr><tr><td align=center><code>position</code></td><td>signless integer</td></tr></tbody></table><h4 id=results-8>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorinsert-vectorinsertop><code>vector.insert</code> (vector::InsertOp)</h3><p>insert operation</p><p>Syntax:</p><pre><code>operation ::= `vector.insert` $source `,` $dest $position attr-dict `:` type($source) `into` type($dest)
</code></pre><p>Takes an n-D source vector, an (n+k)-D destination vector and a k-D position
and inserts the n-D source into the (n+k)-D destination at the proper
position. Degenerates to a scalar source type when n = 0.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%2</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>insert <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>[</span><span class=m>3</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%5</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>insert <span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span><span class=p>[</span><span class=m>3</span><span class=p>,</span> <span class=m>3</span><span class=p>,</span> <span class=m>3</span><span class=p>]</span> <span class=p>:</span> <span class=k>f32</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-4>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>position</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-8>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>source</code></td><td>any type</td></tr><tr><td align=center><code>dest</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-9>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>res</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorinsert_slices-vectorinsertslicesop><code>vector.insert_slices</code> (vector::InsertSlicesOp)</h3><p>vector insert slices operation</p><p>Syntax:</p><pre><code>operation ::= `vector.insert_slices` $vectors `,` $sizes `,` $strides attr-dict `:` type($vectors) `into`
              type(results)
</code></pre><p>Takes a tuple of vector slices and inserts them into the vector result
according to the &lsquo;sizes&rsquo; and &lsquo;strides&rsquo; parameters.</p><p>The arguments &lsquo;sizes&rsquo; and &lsquo;strides&rsquo; represent a specification for
generating the unrolling of &lsquo;vector&rsquo; shape, which has all slices of shape
&lsquo;sizes&rsquo; except for slices at dimension boundaries when &lsquo;vector&rsquo; dimension
sizes are not a multiple of &lsquo;sizes&rsquo;.</p><p>Each slice in &lsquo;vectors&rsquo; is at the tuple element index corresponding to the
linear index of the slice w.r.t the unrolling scheme represented by &lsquo;sizes&rsquo;.
Currently, only unit strides are supported.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extract_slices <span class=nv>%0</span><span class=p>,</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x2x</span><span class=k>f32</span><span class=p>&gt;</span> into tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>

<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>insert_slices <span class=nv>%0</span><span class=p>,</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
  <span class=p>:</span> tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x2x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=c>// Example with partial slices at dimension boundaries.
</span><span class=c></span><span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extract_slices <span class=nv>%2</span><span class=p>,</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>f32</span><span class=p>&gt;</span> into tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span>
                               <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>

<span class=nv>%4</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>insert_slices <span class=nv>%3</span><span class=p>,</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
  <span class=p>:</span> tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span>
          <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-5>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>sizes</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr><tr><td align=center><code>strides</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-9>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vectors</code></td><td>tuple with any combination of vector of any type values values</td></tr></tbody></table><h4 id=results-10>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorinsert_strided_slice-vectorinsertstridedsliceop><code>vector.insert_strided_slice</code> (vector::InsertStridedSliceOp)</h3><p>strided_slice operation</p><p>Syntax:</p><pre><code>operation ::= `vector.insert_strided_slice` $source `,` $dest attr-dict `:` type($source) `into` type($dest)
</code></pre><p>Takes a k-D source vector, an n-D destination vector (n >= k), n-sized
<code>offsets</code> integer array attribute, a k-sized <code>strides</code> integer array attribute
and inserts the k-D source vector as a strided subvector at the proper offset
into the n-D destination vector.</p><p>At the moment strides must contain only 1s.</p><p>Returns an n-D vector that is a copy of the n-D destination vector in which
the last k-D dimensions contain the k-D source vector elements strided at
the proper location as specified by the offsets.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%2</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>insert_strided_slice <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span>
    <span class=p>{</span><span class=nl>offsets =</span> <span class=p>[</span><span class=m>0</span><span class=p>,</span> <span class=m>0</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=nl>strides =</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span><span class=p>}</span><span class=p>:</span>
  <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x4x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x4x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-6>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>offsets</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr><tr><td align=center><code>strides</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-10>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>source</code></td><td>vector of any type values</td></tr><tr><td align=center><code>dest</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-11>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>res</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectormatrix_multiply-vectormatmulop><code>vector.matrix_multiply</code> (vector::MatmulOp)</h3><p>Vector matrix multiplication op that operates on flattened 1-D MLIR vectors</p><p>Syntax:</p><pre><code>operation ::= `vector.matrix_multiply` $lhs `,` $rhs attr-dict `:` `(` type($lhs) `,` type($rhs) `)` `-&gt;` type($res)
</code></pre><p>This is the counterpart of llvm.matrix.multiply in MLIR. It serves the
purposes of more progressive lowering and localized type conversion.</p><p>The ‘vector.matrix_multiply’ op treats <code>lhs</code> as matrix with &lt;lhs_rows> rows
and &lt;lhs_columns> columns, <code>rhs</code> as matrix with &lt;lhs_columns> rows and
&lt;rhs_columns> and multiplies them. The result matrix is returned embedded in
the result vector.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%C</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>matrix_multiply <span class=nv>%A</span><span class=p>,</span> <span class=nv>%B</span>
  <span class=p>{</span> <span class=nl>lhs_rows =</span> <span class=m>4</span><span class=p>:</span> <span class=k>i32</span><span class=p>,</span> <span class=nl>lhs_columns =</span> <span class=m>16</span><span class=p>:</span> <span class=k>i32</span> <span class=p>,</span> <span class=nl>rhs_columns =</span> <span class=m>3</span><span class=p>:</span> <span class=k>i32</span> <span class=p>}</span> <span class=p>:</span>
  <span class=p>(</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>64x</span><span class=k>f64</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>48x</span><span class=k>f64</span><span class=p>&gt;</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>12x</span><span class=k>f64</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-7>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs_rows</code></td><td align=center>IntegerAttr</td><td>32-bit signless integer attribute</td></tr><tr><td align=center><code>lhs_columns</code></td><td align=center>IntegerAttr</td><td>32-bit signless integer attribute</td></tr><tr><td align=center><code>rhs_columns</code></td><td align=center>IntegerAttr</td><td>32-bit signless integer attribute</td></tr></tbody></table><h4 id=operands-11>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>vector of signless integer or signed integer or floating-point values of ranks 1</td></tr><tr><td align=center><code>rhs</code></td><td>vector of signless integer or signed integer or floating-point values of ranks 1</td></tr></tbody></table><h4 id=results-12>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>res</code></td><td>vector of signless integer or signed integer or floating-point values of ranks 1</td></tr></tbody></table><h3 id=vectorouterproduct-vectorouterproductop><code>vector.outerproduct</code> (vector::OuterProductOp)</h3><p>vector outerproduct with optional fused add</p><p>Takes 2 1-D vectors and returns the 2-D vector containing the outer-product.</p><p>An optional extra 2-D vector argument may be specified in which case the
operation returns the sum of the outer-product and the extra vector. In this
multiply-accumulate scenario, the rounding mode is that obtained by
guaranteeing that a fused-multiply add operation is emitted. When lowered to
the LLVMIR dialect, this form emits <code>llvm.intr.fma</code>, which is guaranteed to
lower to actual <code>fma</code> instructions on x86.</p><p>Example:</p><pre><code>%2 = vector.outerproduct %0, %1: vector&lt;4xf32&gt;, vector&lt;8xf32&gt;
return %2: vector&lt;4x8xf32&gt;

%3 = vector.outerproduct %0, %1, %2:
  vector&lt;4xf32&gt;, vector&lt;8xf32&gt;, vector&lt;4x8xf32&gt;
return %3: vector&lt;4x8xf32&gt;
</code></pre><h4 id=operands-12>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>lhs</code></td><td>vector of any type values</td></tr><tr><td align=center><code>rhs</code></td><td>vector of any type values</td></tr><tr><td align=center><code>acc</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-13>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorprint-vectorprintop><code>vector.print</code> (vector::PrintOp)</h3><p>print operation (for testing and debugging)</p><p>Syntax:</p><pre><code>operation ::= `vector.print` $source attr-dict `:` type($source)
</code></pre><p>Prints the source vector (or scalar) to stdout in human readable
format (for testing and debugging). No return value.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>0.0</span> <span class=p>:</span> <span class=k>f32</span>
<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>broadcast <span class=nv>%0</span> <span class=p>:</span> <span class=k>f32</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=kt>vector</span><span class=p>.</span>print <span class=nv>%1</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>f32</span><span class=p>&gt;</span>

when lowered to LLVM<span class=p>,</span> the <span class=kt>vector</span> print is unrolled into
elementary printing method calls that at runtime will yield

<span class=p>(</span> <span class=m>0.0</span><span class=p>,</span> <span class=m>0.0</span><span class=p>,</span> <span class=m>0.0</span><span class=p>,</span> <span class=m>0.0</span> <span class=p>)</span>

on stdout when linked with a small runtime support library<span class=p>,</span>
which only needs to provide a few printing methods <span class=p>(</span>single
value for all data types<span class=p>,</span> opening<span class=err>/</span>closing bracket<span class=p>,</span> comma<span class=p>,</span>
newline<span class=p>)</span><span class=p>.</span>
</code></pre></div><h4 id=operands-13>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>source</code></td><td>any type</td></tr></tbody></table><h3 id=vectorreduction-vectorreductionop><code>vector.reduction</code> (vector::ReductionOp)</h3><p>reduction operation</p><p>Reduces an 1-D vector &ldquo;horizontally&rdquo; into a scalar using the given
operation (add/mul/min/max for int/fp and and/or/xor for int only).
Some reductions (add/mul for fp) also allow an optional fused
accumulator.</p><p>Note that these operations are restricted to 1-D vectors to remain
close to the corresponding LLVM intrinsics:</p><p><a href=http://llvm.org/docs/LangRef.html#experimental-vector-reduction-intrinsics>http://llvm.org/docs/LangRef.html#experimental-vector-reduction-intrinsics</a></p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>reduction <span class=s>&#34;add&#34;</span><span class=p>,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=k>f32</span>

<span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>reduction <span class=s>&#34;xor&#34;</span><span class=p>,</span> <span class=nv>%2</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>&gt;</span> into <span class=k>i32</span>

<span class=nv>%4</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>reduction <span class=s>&#34;mul&#34;</span><span class=p>,</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=k>f32</span>
</code></pre></div><h4 id=attributes-8>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>kind</code></td><td align=center>StringAttr</td><td>string attribute</td></tr></tbody></table><h4 id=operands-14>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr><tr><td align=center><code>acc</code></td><td>any type</td></tr></tbody></table><h4 id=results-14>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>dest</code></td><td>any type</td></tr></tbody></table><h3 id=vectorreshape-vectorreshapeop><code>vector.reshape</code> (vector::ReshapeOp)</h3><p>vector reshape operation</p><p>Syntax:</p><pre><code>operation ::= `vector.reshape` $vector `,` `[` $input_shape `]` `,` `[` $output_shape `]` `,`
              $fixed_vector_sizes attr-dict `:` type($vector) `to` type($result)
</code></pre><p>Reshapes its vector operand from &lsquo;input_shape&rsquo; to &lsquo;output_shape&rsquo; maintaining
fixed vector dimension &lsquo;fixed_vector_sizes&rsquo; on the innermost vector
dimensions.</p><p>The parameters &lsquo;input_shape&rsquo; and &lsquo;output_shape&rsquo; represent valid data shapes
across fixed vector shapes. For example, if a vector has a valid data
shape [6] with fixed vector size [8], then the valid data elements are
assumed to be stored at the beginning of the vector with the remaining
vector elements undefined.</p><p>In the examples below, valid data elements are represented by an alphabetic
character, and undefined data elements are represented by &lsquo;-'.</p><p>Example</p><p>vector&lt;1x8xf32> with valid data shape [6], fixed vector sizes [8]</p><pre><code>        input: [a, b, c, d, e, f]

   layout map: (d0) -&gt; (d0 floordiv 8, d0 mod 8)

vector layout: [a, b, c, d, e, f, -, -]
</code></pre><p>Example</p><p>vector&lt;2x8xf32> with valid data shape [10], fixed vector sizes [8]</p><pre><code>        input: [a, b, c, d, e, f, g, h, i, j]

   layout map: (d0) -&gt; (d0 floordiv 8, d0 mod 8)

vector layout: [[a, b, c, d, e, f, g, h],
                [i, j, -, -, -, -, -, -]]
</code></pre><p>Example</p><p>vector&lt;2x2x2x3xf32> with valid data shape [3, 5], fixed vector sizes
[2, 3]</p><pre><code>        input: [[a, b, c, d, e],
                [f, g, h, i, j],
                [k, l, m, n, o]]

   layout map: (d0, d1) -&gt; (d0 floordiv 3, d1 floordiv 5,
                            d0 mod 3, d1 mod 5)

vector layout: [[[[a, b, c],
                  [f, g, h]]
                 [[d, e, -],
                  [i, j, -]]],
                [[[k, l, m],
                  [-, -, -]]
                 [[n, o, -],
                  [-, -, -]]]]
</code></pre><p>Example</p><p>%1 = vector.reshape %0, [%c3, %c6], [%c2, %c9], [4]
: vector&lt;3x2x4xf32> to vector&lt;2x3x4xf32></p><pre><code>     input: [[a, b, c, d, e, f],
             [g, h, i, j, k, l],
             [m, n, o, p, q, r]]

layout map: (d0, d1) -&gt; (d0, d1 floordiv 4, d1 mod 4)
</code></pre><p>Input vector: [[[a, b, c, d],
[e, f, -, -]],
[[g, h, i, j],
[k, l, -, -]],
[[m, n, o, p],
[q, r, -, -]]]</p><p>Output vector: [[[a, b, c, d],
[e, f, g, h],
[i, -, -, -]],
[[j, k, l, m],
[n, o, p, q],
[r, -, -, -]]]</p><h4 id=attributes-9>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>fixed_vector_sizes</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-15>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr><tr><td align=center><code>input_shape</code></td><td>index</td></tr><tr><td align=center><code>output_shape</code></td><td>index</td></tr></tbody></table><h4 id=results-15>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorshape_cast-vectorshapecastop><code>vector.shape_cast</code> (vector::ShapeCastOp)</h3><p>shape_cast casts between vector shapes</p><p>Syntax:</p><pre><code>operation ::= `vector.shape_cast` $source attr-dict `:` type($source) `to` type($result)
</code></pre><p>The shape_cast operation casts between an n-D source vector shape and
a k-D result vector shape (the element type remains the same).</p><p>If reducing rank (n > k), result dimension sizes must be a product
of contiguous source dimension sizes.
If expanding rank (n &lt; k), source dimensions must factor into a
contiguous sequence of destination dimension sizes.
Each source dim is expanded (or contiguous sequence of source dims combined)
in source dimension list order (i.e. 0 &lt;= i &lt; n), to produce a contiguous
sequence of result dims (or a single result dim), in result dimension list
order (i.e. 0 &lt;= j &lt; k). The product of all source dimension sizes and all
result dimension sizes must match.</p><p>If the source/result types are a tuple of vectors, the casting operation
described above is applied to each source/result tuple element pair.</p><p>It is currently assumed that this operation does not require moving data,
and that it will be folded away before lowering vector operations.</p><p>There is an exception to the folding expectation when targeting
llvm.intr.matrix operations. We need a type conversion back and forth from a
2-D MLIR vector to a 1-D flattened LLVM vector.shape_cast lowering to LLVM
is supported in that particular case, for now.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// Example casting to a lower vector rank.
</span><span class=c></span><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>shape_cast <span class=nv>%0</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>5x1x4x3x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>20x3x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=c>// Example casting to a higher vector rank.
</span><span class=c></span><span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>shape_cast <span class=nv>%2</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>10x12x8x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>5x2x3x4x8x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=c>// Example casting a tuple of vectors of same rank, where tuple elements
</span><span class=c></span><span class=c>// may have different shapes.
</span><span class=c></span><span class=nv>%5</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>shape_cast <span class=nv>%4</span> <span class=p>:</span> tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x3x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span> to
                            tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>12x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>9x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-16>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>source</code></td><td>vector of any type values or tuple with any combination of vector of any type values values</td></tr></tbody></table><h4 id=results-16>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>vector of any type values or tuple with any combination of vector of any type values values</td></tr></tbody></table><h3 id=vectorshuffle-vectorshuffleop><code>vector.shuffle</code> (vector::ShuffleOp)</h3><p>shuffle operation</p><p>The shuffle operation constructs a permutation (or duplication) of elements
from two input vectors, returning a vector with the same element type as
the input and a length that is the same as the shuffle mask. The two input
vectors must have the same element type, rank, and trailing dimension sizes
and shuffles their values in the leading dimension (which may differ in size)
according to the given mask. The legality rules are:</p><ul><li>the two operands must have the same element type as the result</li><li>the two operands and the result must have the same rank and trailing
dimension sizes, viz. given two k-D operands
v1 : &lt;s_1 x s_2 x .. x s_k x type> and
v2 : &lt;t_1 x t_2 x .. x t_k x type>
we have s_i = t_i for all 1 &lt; i &lt;= k</li><li>the mask length equals the leading dimension size of the result</li><li>numbering the input vector indices left to right across the operands, all
mask values must be within range, viz. given two k-D operands v1 and v2
above, all mask values are in the range [0,s_1+t_1)</li></ul><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>shuffle <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>[</span><span class=m>0</span><span class=p>,</span> <span class=m>3</span><span class=p>]</span>
           <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f32</span><span class=p>&gt;</span>       <span class=err>;</span> yields <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>shuffle <span class=nv>%c</span><span class=p>,</span> <span class=nv>%b</span><span class=p>[</span><span class=m>0</span><span class=p>,</span> <span class=m>1</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span>
           <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x16x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>1x16x</span><span class=k>f32</span><span class=p>&gt;</span> <span class=err>;</span> yields <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x16x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%2</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>shuffle <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>[</span><span class=m>3</span><span class=p>,</span> <span class=m>2</span><span class=p>,</span> <span class=m>1</span><span class=p>,</span> <span class=m>0</span><span class=p>]</span>
           <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>f32</span><span class=p>&gt;</span>       <span class=err>;</span> yields <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-10>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>mask</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-17>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>v1</code></td><td>vector of any type values</td></tr><tr><td align=center><code>v2</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-17>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectorstrided_slice-vectorstridedsliceop><code>vector.strided_slice</code> (vector::StridedSliceOp)</h3><p>strided_slice operation</p><p>Syntax:</p><pre><code>operation ::= `vector.strided_slice` $vector attr-dict `:` type($vector) `to` type(results)
</code></pre><p>Takes an n-D vector, k-D <code>offsets</code> integer array attribute, a k-sized
<code>sizes</code> integer array attribute, a k-sized <code>strides</code> integer array
attribute and extracts the n-D subvector at the proper offset.</p><p>At the moment strides must contain only 1s.
// TODO(ntv) support non-1 strides.</p><p>Returns an n-D vector where the first k-D dimensions match the <code>sizes</code>
attribute. The returned subvector contains the elements starting at offset
<code>offsets</code> and ending at <code>offsets + sizes</code>.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>strided_slice <span class=nv>%0</span>
    <span class=p>{</span><span class=nl>offsets =</span> <span class=p>[</span><span class=m>0</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span><span class=p>,</span> <span class=nl>sizes =</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>4</span><span class=p>]</span><span class=p>,</span> <span class=nl>strides =</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span><span class=p>}</span><span class=p>:</span>
  <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x8x16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x4x16x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=c>// TODO(ntv) Evolve to a range form syntax similar to:
</span><span class=c></span><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>strided_slice <span class=nv>%0</span><span class=p>[</span><span class=m>0</span><span class=p>:</span><span class=m>2</span><span class=p>:</span><span class=m>1</span><span class=p>]</span><span class=p>[</span><span class=m>2</span><span class=p>:</span><span class=m>4</span><span class=p>:</span><span class=m>1</span><span class=p>]</span>
  <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x8x16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x4x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-11>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>offsets</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr><tr><td align=center><code>sizes</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr><tr><td align=center><code>strides</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-18>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-18>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>vector of any type values</td></tr></tbody></table><h3 id=vectortransfer_read-vectortransferreadop><code>vector.transfer_read</code> (vector::TransferReadOp)</h3><p>Reads a supervector from memory into an SSA vector value.</p><p>The <code>vector.transfer_read</code> op performs a blocking read from a slice within
a
<a href=/docs/LangRef/#memref-type>MemRef</a>
supplied as its first operand
into a
<a href=/docs/LangRef/#vector-type>vector</a>
of the same base elemental type.</p><p>A memref operand with vector element type, must have its vector element
type match a suffix (shape and element type) of the vector (e.g.
memref&lt;3x2x6x4x3xf32>, vector&lt;1x1x4x3xf32>).</p><p>The slice is further defined by a full-rank index within the MemRef,
supplied as the operands <code>2 .. 1 + rank(memref)</code>. The permutation_map
<a href=/docs/LangRef/#attributes>attribute</a>
is an
<a href=/docs/Dialects/Affine/#affine-maps>affine-map</a>
which specifies the transposition on the
slice to match the vector shape. The size of the slice is specified by the
size of the vector, given as the return type. An <code>ssa-value</code> of the same
elemental type as the MemRef is provided as the last operand to specify
padding in the case of out-of-bounds accesses. This operation is called
&lsquo;read&rsquo; by opposition to &lsquo;load&rsquo; because the super-vector granularity is
generally not representable with a single hardware register.
A <code>vector.transfer_read</code> is thus a mid-level
abstraction that supports super-vectorization with non-effecting padding for
full-tile-only code.</p><p>More precisely, let&rsquo;s dive deeper into the permutation_map for the following
MLIR:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=kt>vector</span><span class=p>.</span>transfer_read <span class=nv>%A</span><span class=p>[</span><span class=nv>%expr1</span><span class=p>,</span> <span class=nv>%expr2</span><span class=p>,</span> <span class=nv>%expr3</span><span class=p>,</span> <span class=nv>%expr4</span><span class=p>]</span>
  <span class=p>{</span> permutation_map <span class=p>:</span> <span class=p>(</span>d0<span class=p>,</span>d1<span class=p>,</span>d2<span class=p>,</span>d3<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>d2<span class=p>,</span><span class=m>0</span><span class=p>,</span>d0<span class=p>)</span> <span class=p>}</span> <span class=p>:</span>
  <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x?x?x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><p>This operation always reads a slice starting at <code>%A[%expr1, %expr2, %expr3, %expr4]</code>. The size of the slice is 3 along d2 and 5 along d0, so the slice
is: <code>%A[%expr1 : %expr1 + 5, %expr2, %expr3:%expr3 + 3, %expr4]</code></p><p>That slice needs to be read into a <code>vector&lt;3x4x5xf32></code>. Since the
permutation map is not full rank, there must be a broadcast along vector
dimension <code>1</code>.</p><p>A notional lowering of vector.transfer_read could generate code resembling:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// %expr1, %expr2, %expr3, %expr4 defined before this point
</span><span class=c></span><span class=nv>%tmp</span> <span class=p>=</span> alloc<span class=p>(</span><span class=p>)</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%view_in_tmp</span> <span class=p>=</span> <span class=s>&#34;element_type_cast&#34;</span><span class=p>(</span><span class=nv>%tmp</span><span class=p>)</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>1x</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>
for <span class=nv>%i</span> <span class=p>=</span> <span class=m>0</span> to <span class=m>3</span> <span class=p>{</span>
  affine<span class=p>.</span>for <span class=nv>%j</span> <span class=p>=</span> <span class=m>0</span> to <span class=m>4</span> <span class=p>{</span>
    affine<span class=p>.</span>for <span class=nv>%k</span> <span class=p>=</span> <span class=m>0</span> to <span class=m>5</span> <span class=p>{</span>
      <span class=nv>%a</span> <span class=p>=</span> load <span class=nv>%A</span><span class=p>[</span><span class=nv>%expr1</span> <span class=err>+</span> <span class=nv>%k</span><span class=p>,</span> <span class=nv>%expr2</span><span class=p>,</span> <span class=nv>%expr3</span> <span class=err>+</span> <span class=nv>%i</span><span class=p>,</span> <span class=nv>%expr4</span><span class=p>]</span> <span class=p>:</span>
        <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x?x?x</span><span class=k>f32</span><span class=p>&gt;</span>
      store <span class=nv>%tmp</span><span class=p>[</span><span class=nv>%i</span><span class=p>,</span> <span class=nv>%j</span><span class=p>,</span> <span class=nv>%k</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=p>}</span><span class=p>}</span><span class=p>}</span>
<span class=nv>%c0</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span>
<span class=nv>%vec</span> <span class=p>=</span> load <span class=nv>%view_in_tmp</span><span class=p>[</span><span class=nv>%c0</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><p>On a GPU one could then map <code>i</code>, <code>j</code>, <code>k</code> to blocks and threads. Notice that
the temporary storage footprint is <code>3 * 5</code> values but <code>3 * 4 * 5</code> values are
actually transferred between <code>%A</code> and <code>%tmp</code>.</p><p>Alternatively, if a notional vector broadcast operation were available, the
lowered code would resemble:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// %expr1, %expr2, %expr3, %expr4 defined before this point
</span><span class=c></span><span class=nv>%tmp</span> <span class=p>=</span> alloc<span class=p>(</span><span class=p>)</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%view_in_tmp</span> <span class=p>=</span> <span class=s>&#34;element_type_cast&#34;</span><span class=p>(</span><span class=nv>%tmp</span><span class=p>)</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>1x</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>
for <span class=nv>%i</span> <span class=p>=</span> <span class=m>0</span> to <span class=m>3</span> <span class=p>{</span>
  affine<span class=p>.</span>for <span class=nv>%k</span> <span class=p>=</span> <span class=m>0</span> to <span class=m>5</span> <span class=p>{</span>
    <span class=nv>%a</span> <span class=p>=</span> load <span class=nv>%A</span><span class=p>[</span><span class=nv>%expr1</span> <span class=err>+</span> <span class=nv>%k</span><span class=p>,</span> <span class=nv>%expr2</span><span class=p>,</span> <span class=nv>%expr3</span> <span class=err>+</span> <span class=nv>%i</span><span class=p>,</span> <span class=nv>%expr4</span><span class=p>]</span> <span class=p>:</span>
      <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x?x?x</span><span class=k>f32</span><span class=p>&gt;</span>
    store <span class=nv>%tmp</span><span class=p>[</span><span class=nv>%i</span><span class=p>,</span> <span class=m>0</span><span class=p>,</span> <span class=nv>%k</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=p>}</span><span class=p>}</span>
<span class=nv>%c0</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span>
<span class=nv>%tmpvec</span> <span class=p>=</span> load <span class=nv>%view_in_tmp</span><span class=p>[</span><span class=nv>%c0</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%vec</span> <span class=p>=</span> broadcast <span class=nv>%tmpvec</span><span class=p>,</span> <span class=m>1</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x4x5x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><p>where <code>broadcast</code> broadcasts from element 0 to all others along the
specified dimension. This time, the temporary storage footprint is <code>3 * 5</code>
values which is the same amount of data as the <code>3 * 5</code> values transferred.
An additional <code>1</code> broadcast is required. On a GPU this broadcast could be
implemented using a warp-shuffle if loop <code>j</code> were mapped to <code>threadIdx.x</code>.</p><p>Syntax</p><pre><code>operation ::= ssa-id `=` `vector.transfer_read` ssa-use-list
  `{` attribute-entry `} :` memref-type `,` vector-type
</code></pre><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// Read the slice `%A[%i0, %i1:%i1+256, %i2:%i2+32]` into vector&lt;32x256xf32&gt;
</span><span class=c></span><span class=c>// and pad with %f0 to handle the boundary case:
</span><span class=c></span><span class=nv>%f0</span> <span class=p>=</span> <span class=kt>constant</span> <span class=m>0.0</span>f <span class=p>:</span> <span class=k>f32</span>
for <span class=nv>%i0</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%0</span> <span class=p>{</span>
  affine<span class=p>.</span>for <span class=nv>%i1</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%1</span> step <span class=m>256</span> <span class=p>{</span>
    affine<span class=p>.</span>for <span class=nv>%i2</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%2</span> step <span class=m>32</span> <span class=p>{</span>
      <span class=nv>%v</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=nv>%A</span><span class=p>[</span><span class=nv>%i0</span><span class=p>,</span> <span class=nv>%i1</span><span class=p>,</span> <span class=nv>%i2</span><span class=p>]</span><span class=p>,</span> <span class=p>(</span><span class=nv>%f0</span><span class=p>)</span>
           <span class=p>{</span>permutation_map<span class=p>:</span> <span class=p>(</span>d0<span class=p>,</span> d1<span class=p>,</span> d2<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>d2<span class=p>,</span> d1<span class=p>)</span><span class=p>}</span> <span class=p>:</span>
           <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x?x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>32x256x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=p>}</span><span class=p>}</span><span class=p>}</span>

<span class=c>// Read the slice `%A[%i0, %i1]` (i.e. the element `%A[%i0, %i1]`) into
</span><span class=c></span><span class=c>// vector&lt;128xf32&gt;. The underlying implementation will require a 1-D vector
</span><span class=c></span><span class=c>// broadcast:
</span><span class=c></span>for <span class=nv>%i0</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%0</span> <span class=p>{</span>
  affine<span class=p>.</span>for <span class=nv>%i1</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%1</span> <span class=p>{</span>
    <span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=nv>%A</span><span class=p>[</span><span class=nv>%i0</span><span class=p>,</span> <span class=nv>%i1</span><span class=p>]</span>
         <span class=p>{</span>permutation_map<span class=p>:</span> <span class=p>(</span>d0<span class=p>,</span> d1<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span><span class=m>0</span><span class=p>)</span><span class=p>}</span> <span class=p>:</span>
         <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>128x</span><span class=k>f32</span><span class=p>&gt;</span>
  <span class=p>}</span>
<span class=p>}</span>

<span class=c>// Read from a memref with vector element type.
</span><span class=c></span><span class=nv>%4</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=nv>%arg1</span><span class=p>[</span><span class=nv>%c3</span><span class=p>,</span> <span class=nv>%c3</span><span class=p>]</span><span class=p>,</span> <span class=nv>%vf0</span>
  <span class=p>{</span><span class=nl>permutation_map =</span> <span class=p>(</span>d0<span class=p>,</span> d1<span class=p>)</span><span class=p>-&gt;</span><span class=p>(</span>d0<span class=p>,</span> d1<span class=p>)</span><span class=p>}</span>
    <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>1x1x4x3x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-12>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>permutation_map</code></td><td align=center>AffineMapAttr</td><td>AffineMap attribute</td></tr></tbody></table><h4 id=operands-19>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>memref</code></td><td>memref of any type values</td></tr><tr><td align=center><code>indices</code></td><td>index</td></tr><tr><td align=center><code>padding</code></td><td>any type</td></tr></tbody></table><h4 id=results-19>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectortransfer_write-vectortransferwriteop><code>vector.transfer_write</code> (vector::TransferWriteOp)</h3><p>The vector.transfer_write op writes a supervector to memory.</p><p>Syntax:</p><pre><code>operation ::= `vector.transfer_write` $vector `,` $memref `[` $indices `]` attr-dict `:` type($vector) `,`
              type($memref)
</code></pre><p>The <code>vector.transfer_write</code> performs a blocking write from a
<a href=/docs/LangRef/#vector-type>vector</a>
, supplied as its first operand, into a
slice within a
<a href=/docs/LangRef/#memref-type>MemRef</a>
of the same base
elemental type, supplied as its second operand.</p><p>A vector memref operand must have its vector element type match a suffix
(shape and element type) of the vector (e.g. memref&lt;3x2x6x4x3xf32>,
vector&lt;1x1x4x3xf32>).</p><p>The slice is further defined by a full-rank index within the MemRef,
supplied as the operands <code>3 .. 2 + rank(memref)</code>.
The permutation_map
<a href=/docs/LangRef/#attributes>attribute</a>
is an
<a href=/docs/Dialects/Affine/#affine-maps>affine-map</a>
which specifies the transposition on the
slice to match the vector shape. The size of the slice is specified by the
size of the vector. This operation is called &lsquo;write&rsquo; by opposition to
&lsquo;store&rsquo; because the super-vector granularity is generally not representable
with a single hardware register. A <code>vector.transfer_write</code> is thus a
mid-level abstraction that supports super-vectorization with non-effecting
padding for full-tile-only code. It is the responsibility of
<code>vector.transfer_write</code>'s implementation to ensure the memory writes are
valid. Different lowerings may be pertinent depending on the hardware
support.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// write vector&lt;16x32x64xf32&gt; into the slice
</span><span class=c></span><span class=c>//   `%A[%i0, %i1:%i1+32, %i2:%i2+64, %i3:%i3+16]`:
</span><span class=c></span>for <span class=nv>%i0</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%0</span> <span class=p>{</span>
  affine<span class=p>.</span>for <span class=nv>%i1</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%1</span> step <span class=m>32</span> <span class=p>{</span>
    affine<span class=p>.</span>for <span class=nv>%i2</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%2</span> step <span class=m>64</span> <span class=p>{</span>
      affine<span class=p>.</span>for <span class=nv>%i3</span> <span class=p>=</span> <span class=m>0</span> to <span class=nv>%3</span> step <span class=m>16</span> <span class=p>{</span>
        <span class=nv>%val</span> <span class=p>=</span> <span class=err>`</span>ssa<span class=err>-</span>value<span class=err>`</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x32x64x</span><span class=k>f32</span><span class=p>&gt;</span>
        <span class=kt>vector</span><span class=p>.</span>transfer_write <span class=nv>%val</span><span class=p>,</span> <span class=nv>%A</span><span class=p>[</span><span class=nv>%i0</span><span class=p>,</span> <span class=nv>%i1</span><span class=p>,</span> <span class=nv>%i2</span><span class=p>,</span> <span class=nv>%i3</span><span class=p>]</span>
          <span class=p>{</span>permutation_map<span class=p>:</span> <span class=p>(</span>d0<span class=p>,</span> d1<span class=p>,</span> d2<span class=p>,</span> d3<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>d3<span class=p>,</span> d1<span class=p>,</span> d2<span class=p>)</span><span class=p>}</span> <span class=p>:</span>
          <span class=kt>vector</span><span class=p>&lt;</span><span class=m>16x32x64x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x?x?x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=p>}</span><span class=p>}</span><span class=p>}</span><span class=p>}</span>

<span class=c>// write to a memref with vector element type.
</span><span class=c></span><span class=kt>vector</span><span class=p>.</span>transfer_write <span class=nv>%4</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>[</span><span class=nv>%c3</span><span class=p>,</span> <span class=nv>%c3</span><span class=p>]</span>
  <span class=p>{</span><span class=nl>permutation_map =</span> <span class=p>(</span>d0<span class=p>,</span> d1<span class=p>)</span><span class=p>-&gt;</span><span class=p>(</span>d0<span class=p>,</span> d1<span class=p>)</span><span class=p>}</span>
    <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>1x1x4x3x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x3x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-13>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>permutation_map</code></td><td align=center>AffineMapAttr</td><td>AffineMap attribute</td></tr></tbody></table><h4 id=operands-20>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr><tr><td align=center><code>memref</code></td><td>memref of any type values</td></tr><tr><td align=center><code>indices</code></td><td>index</td></tr></tbody></table><h3 id=vectortranspose-vectortransposeop><code>vector.transpose</code> (vector::TransposeOp)</h3><p>vector transpose operation</p><p>Syntax:</p><pre><code>operation ::= `vector.transpose` $vector `,` $transp attr-dict `:` type($vector) `to` type($result)
</code></pre><p>Takes a n-D vector and returns the transposed n-D vector defined by
the permutation of ranks in the n-sized integer array attribute.
In the operation</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transpose <span class=nv>%0</span><span class=p>,</span> <span class=p>[</span>i_1<span class=p>,</span> <span class=p>.</span><span class=p>.</span><span class=p>,</span> i_n<span class=p>]</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span>d_1 <span class=p>x</span> <span class=p>.</span><span class=p>.</span> <span class=p>x</span> d_n <span class=p>x</span> <span class=k>f32</span><span class=p>&gt;</span>
  to <span class=kt>vector</span><span class=p>&lt;</span>d_trans<span class=p>[</span><span class=m>0</span><span class=p>]</span> <span class=p>x</span> <span class=p>.</span><span class=p>.</span> <span class=p>x</span> d_trans<span class=p>[</span>n<span class=m>-1</span><span class=p>]</span> <span class=p>x</span> <span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><p>the transp array [i_1, .., i_n] must be a permutation of [0, .., n-1].</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transpose <span class=nv>%0</span><span class=p>,</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>0</span><span class=p>]</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>vector</span><span class=p>&lt;</span><span class=m>3x2x</span><span class=k>f32</span><span class=p>&gt;</span>

 <span class=p>[</span> <span class=p>[</span>a<span class=p>,</span> b<span class=p>,</span> c<span class=p>]</span><span class=p>,</span>       <span class=p>[</span> <span class=p>[</span>a<span class=p>,</span> d<span class=p>]</span><span class=p>,</span>
   <span class=p>[</span>d<span class=p>,</span> e<span class=p>,</span> f<span class=p>]</span> <span class=p>]</span>  <span class=p>-&gt;</span>    <span class=p>[</span>b<span class=p>,</span> e<span class=p>]</span><span class=p>,</span>
                      <span class=p>[</span>c<span class=p>,</span> f<span class=p>]</span> <span class=p>]</span>
</code></pre></div><h4 id=attributes-14>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>transp</code></td><td align=center>ArrayAttr</td><td>64-bit integer array attribute</td></tr></tbody></table><h4 id=operands-21>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vector</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-20>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>result</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=vectortuple_get-vectortuplegetop><code>vector.tuple_get</code> (vector::TupleGetOp)</h3><p>vector tuple get operation</p><p>Returns the tuple element of &lsquo;vectors&rsquo; at &lsquo;index&rsquo;.</p><p>Note that this operation is used during the vector op unrolling
transformation and should be removed before lowering to lower-level
dialects.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%4</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>tuple <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>,</span> <span class=nv>%3</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>

<span class=nv>%5</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>tuple_get <span class=nv>%4</span><span class=p>,</span> <span class=m>1</span>
  <span class=p>:</span> tuple<span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span>
          <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-15>Attributes:</h4><table><thead><tr><th align=center>Attribute</th><th align=center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>index</code></td><td align=center>IntegerAttr</td><td>arbitrary integer attribute</td></tr></tbody></table><h4 id=operands-22>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vectors</code></td><td>tuple with any combination of vector of any type values values</td></tr></tbody></table><h4 id=results-21>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>vector of any type values</td></tr></tbody></table><h3 id=vectortuple-vectortupleop><code>vector.tuple</code> (vector::TupleOp)</h3><p>make tuple of vectors operation</p><p>Returns a tuple of its operands &lsquo;vectors&rsquo;.</p><p>Note that this operation is used during the vector op unrolling
transformation and should be removed before lowering to lower-level
dialects.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=p>.</span><span class=p>.</span><span class=p>.</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=p>.</span><span class=p>.</span><span class=p>.</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%2</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=p>.</span><span class=p>.</span><span class=p>.</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%3</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>transfer_read <span class=p>.</span><span class=p>.</span><span class=p>.</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span>

<span class=nv>%4</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>tuple <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>,</span> <span class=nv>%3</span>
  <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-23>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>vectors</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-22>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>tuple with any combination of vector of any type values values</td></tr></tbody></table><h3 id=vectortype_cast-vectortypecastop><code>vector.type_cast</code> (vector::TypeCastOp)</h3><p>type_cast op converts a scalar memref to a vector memref</p><p>Performs a conversion from a memref with scalar element to a memref with a
<em>single</em> vector element, copying the shape of the memref to the vector. This
is the minimal viable operation that is required to makeke
super-vectorization operational. It can be seen as a special case of the
<code>view</code> operation but scoped in the super-vectorization context.</p><p>Syntax:</p><pre><code>operation ::= `vector.type_cast` ssa-use : memref-type to memref-type
</code></pre><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%A</span>  <span class=p>=</span> alloc<span class=p>(</span><span class=p>)</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>5x4x3x</span><span class=k>f32</span><span class=p>&gt;</span>
<span class=nv>%VA</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>type_cast <span class=nv>%A</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>5x4x3x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>5x4x3x</span><span class=k>f32</span><span class=p>&gt;</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-24>Operands:</h4><table><thead><tr><th align=center>Operand</th><th>Description</th></tr></thead><tbody><tr><td align=center><code>memref</code></td><td>statically shaped memref of any type values</td></tr></tbody></table><h4 id=results-23>Results:</h4><table><thead><tr><th align=center>Result</th><th>Description</th></tr></thead><tbody><tr><td align=center>«unnamed»</td><td>memref of any type values</td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Dialects/Standard/ title="'std' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - 'std' Dialect</a>
<a class="nav nav-next" href=/docs/Rationale/ title=Rationale>Next - Rationale <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/talks/>Talks and Related Publications</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class="parent has-sub-menu"><a href=/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=/docs/Dialects/Linalg/>'linalg' Dialect</a></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/LoopDialect/>'loop' Dialect</a></li><li><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>'spv' Dialect</a></li><li><a href=/docs/Dialects/Standard/>'std' Dialect</a></li><li class=active><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li></ul></li><li class=has-sub-menu><a href=/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Rationale/UsageOfConst/>Usage of 'Const' in MLIR, for core IR types</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Tutorial Introduction</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/Tutorials/DefiningAttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li></ul></li><li><a href=/docs/EDSC/>Background: declarative builders API</a></li><li><a href=/docs/ConversionToLLVMDialect/>Conversion to the LLVM Dialect</a></li><li><a href=/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li><a href=/docs/GenericDAGRewriter/>MLIR Generic DAG Rewriter Infrastructure</a></li><li><a href=/docs/Interfaces/>MLIR Interfaces</a></li><li><a href=/docs/LangRef/>MLIR Language Reference</a></li><li><a href=/docs/Traits/>MLIR Operation Traits</a></li><li><a href=/docs/Passes/>MLIR Passes</a></li><li><a href=/docs/Quantization/>MLIR Quantization</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=/docs/ShapeInference/>Shape Inference</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/OpDefinitions/>Table-driven Operation Definition Specification (ODS)</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>